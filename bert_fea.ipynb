{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import pickle\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# 节省内存的一个函数\n",
    "# 减少内存\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,100*(start_mem-end_mem)/start_mem,(time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data_v2/'\n",
    "save_path = './5000_sample/'\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  7.56 Mb (44.8% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to  0.10 Mb (0.0% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to 154.78 Mb (54.7% reduction),time spend:0.01 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\system_default\\desktop\\kaggle\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\system_default\\desktop\\kaggle\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\system_default\\desktop\\kaggle\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\system_default\\desktop\\kaggle\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\system_default\\desktop\\kaggle\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5596413, 8) (5425504, 8)\n"
     ]
    }
   ],
   "source": [
    "vid_info = pd.read_csv(data_path + 'vid_info.csv')\n",
    "candidate_items = pd.read_csv(data_path + 'candidate_items_A.csv')\n",
    "seq_train = pd.read_csv(data_path + 'main_vv_seq_train.csv')\n",
    "\n",
    "vid_info = reduce_mem(vid_info)\n",
    "candidate_items = reduce_mem(candidate_items)\n",
    "seq_train = reduce_mem(seq_train)\n",
    "\n",
    "# 视频库标签编码 , 这里的 训练和测试都一样\n",
    "data_did_lb = LabelEncoder()\n",
    "vid_info_lb = LabelEncoder()\n",
    "cid_info_lb = LabelEncoder()\n",
    "\n",
    "seq_train['did'] = data_did_lb.fit_transform(seq_train[['did']])\n",
    "\n",
    "vid_info['vid'] = vid_info_lb.fit_transform(vid_info[['vid']])\n",
    "vid_info['cid'] = cid_info_lb.fit_transform(vid_info[['cid']])\n",
    "\n",
    "seq_train['vid'] = vid_info_lb.transform(seq_train[['vid']])\n",
    "candidate_items['vid'] = vid_info_lb.transform(candidate_items[['vid']])\n",
    "\n",
    "vid_info['stars'] = vid_info['stars'].apply(eval)\n",
    "vid_info['tags'] = vid_info['tags'].apply(eval)\n",
    "vid_info['key_word'] = vid_info['key_word'].apply(eval)\n",
    "\n",
    "vid_info['stars'] = vid_info['stars'].apply(set)\n",
    "vid_info['tags'] = vid_info['tags'].apply(set)\n",
    "vid_info['key_word'] = vid_info['key_word'].apply(set)\n",
    "\n",
    "# 获取当前数据的历史点击和最后一次点击\n",
    "def get_test_train(train_):\n",
    "    \n",
    "    train_.sort_values(by=['did','seq_no'],inplace=True,ascending=False)\n",
    "    train_['site'] =  train_.groupby('did').cumcount()+1\n",
    "    \n",
    "    local_final_log = train_[train_['site'] == 1].reset_index(drop=True)\n",
    "    train_d = train_[train_['site'] != 1].reset_index(drop=True)\n",
    "\n",
    "    del local_final_log['site']\n",
    "    del train_d['site']\n",
    "\n",
    "    return local_final_log,train_d\n",
    "\n",
    "# 划分数据集\n",
    "local_final_log,train_data = get_test_train(seq_train.copy()) \n",
    "all_data = seq_train \n",
    "\n",
    "del seq_train\n",
    "print(all_data.shape,train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_vid = candidate_items['vid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(cand_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat,get_feature_names\n",
    "from deepctr.models import DSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_fd(df, dense_fea, sparse_fea, behavior_fea, emb_dim=16, max_len=10, hash_flag=False):\n",
    "\n",
    "    # 稀疏型特征\n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=df[feat].nunique() + 1, embedding_dim=emb_dim) for feat in sparse_fea]\n",
    "    # 稠密型特征\n",
    "    dense_feature_columns = [DenseFeat(feat, 1, ) for feat in dense_fea]\n",
    "\n",
    "    feature_columns = sparse_feature_columns + dense_feature_columns \n",
    "    # 以最近两小时为 session 0 \n",
    "    feature_columns += [ \n",
    "        VarLenSparseFeat(SparseFeat('sess_0_vid', vocabulary_size=df['candi_vid'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='candi_vid'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_cid', vocabulary_size=df['cid'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='cid'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_is_intact', vocabulary_size=df['is_intact'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='is_intact'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_classify_id', vocabulary_size=df['classify_id'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='classify_id'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_series_id', vocabulary_size=df['series_id'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='series_id'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_cpn', vocabulary_size=df['cpn'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='cpn'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_fpn', vocabulary_size=df['fpn'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='fpn'),\n",
    "                         maxlen=10),\n",
    "        ]\n",
    "    # 以两小时后的历史记录 为 session 1\n",
    "    feature_columns += [\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_vid', vocabulary_size=df['candi_vid'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='candi_vid'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_cid', vocabulary_size=df['cid'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='cid'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_is_intact', vocabulary_size=df['is_intact'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='is_intact'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_classify_id', vocabulary_size=df['classify_id'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='classify_id'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_series_id', vocabulary_size=df['series_id'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='series_id'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_cpn', vocabulary_size=df['cpn'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='cpn'),\n",
    "                         maxlen=10),\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_fpn', vocabulary_size=df['fpn'].nunique() + 1, embedding_dim=emb_dim, use_hash=hash_flag, embedding_name='fpn'),\n",
    "                         maxlen=10),\n",
    "        ]\n",
    "\n",
    "    sess_number = np.array([2, 1, 0])\n",
    "\n",
    "    feature_dict = {'fpn_score','nen_score','next_score','vid_pop','vid_pop_7','vid_pop_2','wr_mean','fr_mean',\n",
    "       'wr','fr','time_diff','wr_favor','fr_favor','dura_mean','dura_max','dura_min','til_mean',\n",
    "       'cid_s','isi_s','cla_s','ser_s','stars_sim','tags_sim','key_word_sim','duration_s','vid_pop_s','title_s',\n",
    "       'candi_vid','cid','serialno','is_intact','classify_id','series_id','cpn','fpn',\n",
    "       'sess_0_vid','sess_0_cid','sess_0_is_intact','sess_0_classify_id','sess_0_series_id','sess_0_cpn','sess_0_fpn',\n",
    "       'sess_1_vid','sess_1_cid','sess_1_is_intact','sess_1_classify_id','sess_1_series_id','sess_1_cpn','sess_1_fpn',\n",
    "    }\n",
    "\n",
    "\n",
    "    # x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}\n",
    "    x = { get_feature_names(feat) for feat in feature_dict }\n",
    "    x[\"sess_length\"] = 2\n",
    "\n",
    "    return x, feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_x, feature_columns = get_xy_fd(trn_data, dense_fea, sparse_fea, behavior_fea, emb_dim=16, max_len=10 ,hash_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = np.array([0, 1, 2])\n",
    "ugender = np.array([0, 1, 0])\n",
    "iid = np.array([1, 2, 3])  # 0 is mask value\n",
    "cateid = np.array([1, 2, 2])  # 0 is mask value\n",
    "score = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "sess1_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [0, 0, 0, 0]])\n",
    "sess1_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [0, 0, 0, 0]])\n",
    "\n",
    "sess2_iid = np.array([[1, 2, 3, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "sess2_cate_id = np.array([[1, 2, 2, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "sess_number = np.array([2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 0],\n",
       "       [3, 2, 1, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess1_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [0, 0, 0, 0]])\n",
    "sess1_iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess2_iid = np.array([[1, 2, 3, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "sess2_iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat,get_feature_names\n",
    "from deepctr.models import DSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把特征分开\n",
    "sparse_fea = ['did','candi_vid','cid','is_intact','classify_id','series_id','cpn','fpn']\n",
    "\n",
    "dense_fea = ['fpn_score','nen_score','next_score','vid_pop_7','vid_pop_2','wr_mean','fr_mean',\n",
    "       'wr','fr','wr_favor','fr_favor','dura_mean','dura_max',\n",
    "       'dura_min','til_mean','cid_s','isi_s','cla_s','ser_s','stars_sim',\n",
    "       'tags_sim','key_word_sim','vid_emb',\n",
    "       ]\n",
    "\n",
    "behavior_fea = ['cid','classify_id','is_intact','series_id','cpn','fpn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "model = DSIN(feature_columns, behavior_fea, sess_max_count=2,\n",
    "              dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5, )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy',\n",
    "              metrics=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat,get_feature_names\n",
    "from deepctr.models import DSIN\n",
    "\n",
    "\n",
    "def get_xy_fd(hash_flag=False):\n",
    "    feature_columns = [SparseFeat('user', 3, embedding_dim=10, use_hash=hash_flag),\n",
    "                       SparseFeat('gender', 2, embedding_dim=4, use_hash=hash_flag),\n",
    "                       SparseFeat('item', 3 + 1, embedding_dim=4, use_hash=hash_flag),\n",
    "                       SparseFeat('cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag),\n",
    "                       DenseFeat('pay_score', 1)]\n",
    "    feature_columns += [\n",
    "        VarLenSparseFeat(SparseFeat('sess_0_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),\n",
    "                         maxlen=4), VarLenSparseFeat(\n",
    "            SparseFeat('sess_0_cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='cate_id'),\n",
    "            maxlen=4)]\n",
    "    feature_columns += [\n",
    "        VarLenSparseFeat(SparseFeat('sess_1_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),\n",
    "                         maxlen=4), \n",
    "                         VarLenSparseFeat(\n",
    "            SparseFeat('sess_1_cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='cate_id'),\n",
    "            maxlen=4)]\n",
    "\n",
    "    behavior_feature_list = [\"item\", \"cate_id\"]\n",
    "    uid = np.array([0, 1, 2])\n",
    "    ugender = np.array([0, 1, 0])\n",
    "    iid = np.array([1, 2, 3])  # 0 is mask value\n",
    "    cateid = np.array([1, 2, 2])  # 0 is mask value\n",
    "    score = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "    sess1_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [0, 0, 0, 0]])\n",
    "    sess1_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [0, 0, 0, 0]])\n",
    "\n",
    "    sess2_iid = np.array([[1, 2, 3, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "    sess2_cate_id = np.array([[1, 2, 2, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "    sess_number = np.array([2, 1, 0])\n",
    "\n",
    "    feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'cate_id': cateid,\n",
    "                    'sess_0_item': sess1_iid, 'sess_0_cate_id': sess1_cate_id, 'pay_score': score,\n",
    "                    'sess_1_item': sess2_iid, 'sess_1_cate_id': sess2_cate_id, }\n",
    "\n",
    "    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}\n",
    "    x[\"sess_length\"] = sess_number\n",
    "    y = np.array([1, 0, 1])\n",
    "    return x, y, feature_columns, behavior_feature_list\n",
    "\n",
    "\n",
    "\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "x, y, feature_columns, behavior_feature_list = get_xy_fd(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DSIN(feature_columns, behavior_feature_list, sess_max_count=2,\n",
    "                dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_emb_size = list(\n",
    "        map(lambda fc: fc.embedding_dim, filter(lambda fc: fc.name in behavior_feature_list, feature_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item', 'cate_id']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'binary_crossentropy',\n",
    "                metrics=['binary_crossentropy'])\n",
    "history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "407107e68832157a7bf440b2e842437834c5ebf88d5cafbcbfa00fd7b9d9b746"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
