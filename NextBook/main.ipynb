{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 引入第三方类 声明全局变量 \n",
    "import random\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Linear, Embedding\n",
    "import paddle.nn.functional as F\n",
    "import math\n",
    "import pickle \n",
    "import difflib\n",
    "\n",
    "# 声明每个数据文件的路径\n",
    "user_info_path = \"lrh/userdata.dat\"\n",
    "rating_info_path = \"lrh/ratingdata.dat\"\n",
    "book_info_path = \"lrh/bookdata.dat\"\n",
    "book_train_feat = \"lrh/book_train_feat.pkl\"\n",
    "user_train_feat = \"lrh/user_train_feat.pkl\"\n",
    "book_raw_feat = \"lrh/book_raw_feat.pkl\"\n",
    "user_raw_feat = \"lrh/user_raw_feat.pkl\"\n",
    "\n",
    "# 定义图书的类别大小\n",
    "cat_size = 4\n",
    "\n",
    "# 定义数据迭代Batch大小\n",
    "BATCHSIZE = 256\n",
    "\n",
    "# 队列中的最大值\n",
    "def list_max(list):\n",
    "    a = list[0]\n",
    "    for prime in range (1,len(list)):\n",
    "        if list[prime] > list[0]:\n",
    "         a = list[prime]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 优化图书数据\n",
    "# import random\n",
    "# # 打开文件，编码方式选择utf-8，读取所有数据到data中\n",
    "# with open('data/data7492/bookdata.dat', 'r', encoding=\"UTF-8-sig\") as f:\n",
    "#     data = f.readlines()\n",
    "\n",
    "# all_cats = []  # 图书类别字典\n",
    "# book_cats = {}  # 图书类别字典\n",
    "# cats_book = {} # 图书类别字典\n",
    "# cat_count = 0 # 图书类别数量\n",
    "# cats_list = {} \n",
    "\n",
    "# # 遍历原始数据\n",
    "# for item in data:\n",
    "#     item = item.strip().split(\"::\")\n",
    "\n",
    "#     # 获取图书类别\n",
    "#     cats = item[2].split(' ')\n",
    "#     for cat in cats :\n",
    "#         all_cats.append(cat)\n",
    "\n",
    "# for cat in all_cats:\n",
    "#     if cat not in cats_list.keys():\n",
    "#         cats_list[cat] = all_cats.count(cat)\n",
    "\n",
    "# while len(cats_list) > 0 :\n",
    "#     max_cat = 0\n",
    "#     for cat in cats_list:\n",
    "#         max_cat = max(max_cat,cats_list[cat])\n",
    "\n",
    "#     cats = []\n",
    "#     for cat in cats_list:\n",
    "#         if cats_list[cat] == max_cat:\n",
    "#             cat_count += 1\n",
    "#             cats.append(cat)\n",
    "#             book_cats[cat] = cat_count\n",
    "#             cats_book[cat_count] = cat\n",
    "#     for cat in cats:\n",
    "#         cats_list.pop(cat)\n",
    "\n",
    "# # 打开文件，编码方式选择utf-8，读取所有数据到data中\n",
    "# with open('data/data7492/bookdata.dat', 'r', encoding=\"UTF-8-sig\") as f:\n",
    "#     data = f.readlines()\n",
    "\n",
    "# # 统计图书ID信息、图书标题、图书类别，并给每个类别一个数字序号。\n",
    "# book_info = {} # 图书信息字典\n",
    "# book_codes = {} # 图书编码字典\n",
    "# book_count = 0 # 图书数量\n",
    "\n",
    "# # 遍历原始数据\n",
    "# for item in data:\n",
    "#     item = item.strip().split(\"::\")\n",
    "    \n",
    "#     # 定义图书ID\n",
    "#     book_code = item[0]\n",
    "#     # 定义图书标题\n",
    "#     book_title = item[1]\n",
    "#     # 获取图书类别\n",
    "#     cats = item[2].split(' ')\n",
    "\n",
    "#     book_cat = [int(book_cats[k]) for k in cats]\n",
    "#     book_cat.sort()\n",
    "#     book_cat = book_cat[:4]      \n",
    "#     cats = [cats_book[k] for k in book_cat]\n",
    "\n",
    "#     book_count += 1\n",
    "#     book_id = book_count\n",
    "\n",
    "#     # 保存所有图书数据到字典中\n",
    "#     book_codes[book_code] = str(book_id)\n",
    "#     book_info[str(book_id)] = {'book_id': str(book_id),'book_title': book_title,'cats': cats}\n",
    "\n",
    "# file = open('lrh/bookdata.dat','w')\n",
    "# for k in book_info.keys():\n",
    "#     file.write(book_info[k]['book_id'])\n",
    "#     file.write('::')\n",
    "#     file.write(book_info[k]['book_title'])\n",
    "#     file.write('::')\n",
    "#     for cat in book_info[k]['cats']:\n",
    "#         file.write(cat)\n",
    "#         file.write(' ')\n",
    "#     file.write('\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 优化用户数据\n",
    "\n",
    "# # 打开文件，编码方式选择utf-8，读取所有数据到data中\n",
    "# with open('data/data7492/userdata.dat', 'r', encoding=\"utf-8\") as f:\n",
    "#     data = f.readlines()\n",
    "# # 从评分数据中提炼出用户信息、把用户的已评分图书作为特征。\n",
    "# user_info = {} # 用户信息字典\n",
    "# users = {} # 用户字典\n",
    "# user_count = 0 # 用户数量\n",
    "# user_names = {}\n",
    "\n",
    "# for item in data:\n",
    "\n",
    "#     if user_count > 51415:\n",
    "#         break\n",
    "  \n",
    "#     item = item.strip().split(\"::\")\n",
    "\n",
    "#     user_name = item[0]\n",
    "#     gender = ['男', '女']\n",
    "#     gender = random.choice(gender)\n",
    "#     job = ['医生', '教师', '司机', '工人', '记者', '演员', '厨师', '护士', '商人', '会计师', '职员', '作家', '导游', '模特', '警察', '歌手', '画家', '消防员', '服务员', '清洁工', '工程师', '设计师', '裁缝', '翻译', '法官', '保安', '园丁', '推销员', '行政主管', '企业主管', '经理人', '土木营造监工', '天文学家', '电脑程式设计人员', '系统分析师', '道景师', '建筑师', '交通规划师', '化学工程技术师', '土木工程师', '造景师', '测量员', '销售工程师', '工业工程师', '品质管制工程师', '陶瓷技师', '药师', '兽医师', '公共卫生医师', '中医师', '护理佐理员', '护理师', '学前教育教师', '特殊教育教师', '会计师', '公共关系员', '律师', '心理谘商师', '社会工作师', '人事管理师', '翻译人员', '人力仲介师', '图书管理员', '新闻记者', '报刊杂志编辑人员', '编剧', '书籍编辑', '石雕工', '木雕工', '漫画家', '舞蹈家', '乐器演奏家', '营建及工程管理', '钢结构设计与管理人员', '微电脑', '电子报/电子杂志编辑', '资讯管理师', '幼稚教育教师', '连锁加盟店店长', '木模工', '家俱木工', '缝纫工', '织布工', '西装工', '国服缝制人员', '制鞋工', '服装设计与制作人员', '女装工', '成衣工', '重机械修护工', '保全工作人员', '警察', '消防工作人员', '时装模特儿', '售货员', '商品售货员', '行销企划', '电信工程师', '宠物美容师', '园艺作物栽培员', '花匠', '建筑工', '邮务士', '飞航管制员', '营养师', '银行行员', '邮政工作人员', '眼镜专业人员', '商船工作人员', '摄影工作人员', '病历管理师', '民航运输驾驶员', '网页设计师', '制程工程师', '土木营建机械操作工', '金融业柜员', '油漆工', '厨师', '推销员']\n",
    "#     job = random.choice(job)\n",
    "#     age = random.randint(20, 75)\n",
    "\n",
    "#     #把过滤后的用户信息保存到用户字典\n",
    "#     if user_name not in users:\n",
    "#         user_count += 1\n",
    "#         users[user_name] = user_count\n",
    "#         user_id = user_count\n",
    "\n",
    "#         user_names[user_name] = str(user_id)\n",
    "#         user_info[str(user_id)] = {'user_id': str(user_id),'user_name': user_name, 'gender':gender, 'age':str(age), 'job': job}\n",
    "        \n",
    "# file = open('lrh/userdata.dat','w')\n",
    "# for k in user_info.keys():\n",
    "#     file.write(user_info[k]['user_id'])\n",
    "#     file.write('::')\n",
    "#     file.write(user_info[k]['gender'])\n",
    "#     file.write('::')\n",
    "#     file.write(user_info[k]['age'])\n",
    "#     file.write('::')\n",
    "#     file.write(user_info[k]['job'])\n",
    "#     file.write('\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 生成相似图书数据\n",
    "# import time\n",
    "\n",
    "# book_info , _, _ = get_book_info(book_info_path, True)\n",
    "\n",
    "# similar_books = {}\n",
    "# sim_books = []\n",
    "\n",
    "# for book_id in book_info.keys():\n",
    "#     if int(book_id) % 2000 == 0:\n",
    "#         print(\"处理图书{}本  {}\".format(book_id,time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n",
    "#     if book_id in sim_books:\n",
    "#         continue\n",
    "\n",
    "#     sims = []\n",
    "#     sim_num = 0\n",
    "#     for bid in book_info.keys():\n",
    "        \n",
    "#         sim = difflib.SequenceMatcher(None,book_info[book_id]['book_cat'],book_info[bid]['book_cat'])\n",
    "#         if sim.ratio() >= 0.75 :\n",
    "#             sim_num += 1\n",
    "#         sims.append(sim.ratio())\n",
    "\n",
    "#     if sim_num < 50:\n",
    "#         continue\n",
    "\n",
    "#     # 对相似度排序\n",
    "#     index = np.argsort(sims)[-sim_num:]\n",
    "#     # index = list(reversed(index))\n",
    "#     similar_books[book_id] = []\n",
    "#     for i in index :\n",
    "#         sim_books.append(str(i + 1))\n",
    "#         similar_books[book_id].append(str(i + 1))\n",
    "#     sim_books = list(set(sim_books))\n",
    "\n",
    "# file = open('lrh/similarbooks.dat','w')\n",
    "# for book_id in similar_books.keys():\n",
    "#     file.write(book_id)\n",
    "#     file.write('::')\n",
    "#     for bid in similar_books[book_id]:\n",
    "#         file.write(bid)\n",
    "#         file.write(' ')\n",
    "#     file.write('\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 优化评分数据\n",
    "\n",
    "# # 读取相似图书数据文件\n",
    "# with open('lrh/similarbooks.dat', 'r', encoding=\"UTF-8-sig\") as f:\n",
    "#     data = f.readlines()\n",
    "\n",
    "# similar_books = {}\n",
    "\n",
    "# # 遍历原始数据\n",
    "# for item in data:\n",
    "\n",
    "#     item = item.strip().split(\"::\")\n",
    "    \n",
    "#     book_id = item[0]\n",
    "#     sim_books = item[1].split(' ')\n",
    "    \n",
    "#     similar_books[book_id] = sim_books\n",
    "\n",
    "# for book_id in similar_books.keys():\n",
    "#     if len(similar_books[book_id]) > 100:\n",
    "#         similar_books[book_id] = random.sample(similar_books[str(book_id)], 100)\n",
    "\n",
    "# user_info , _, _ = get_user_info(user_info_path, True)\n",
    "\n",
    "# rating_info = {}\n",
    "# rating_count = 0\n",
    "\n",
    "# book_ids = list(similar_books.keys())\n",
    "# book_ids = random.sample(book_ids, 150)\n",
    "\n",
    "# for user_id in user_info.keys() :\n",
    "\n",
    "#     book_id = random.choice(book_ids)\n",
    "\n",
    "#     rating_num = random.randint(18,25)\n",
    "\n",
    "#     books = random.sample(similar_books[str(book_id)], rating_num)\n",
    "    \n",
    "#     for i in books: \n",
    "#         rating_count += 1\n",
    "#         score = random.choice([5,5,5,5,5,5,5,5,5,4,4,3])\n",
    "\n",
    "#         rating_info[str(rating_count)] = {'user_id':str(user_id),'book_id':i ,'score':str(score)}\n",
    "\n",
    "# file = open('lrh/ratingdata.dat','w')\n",
    "# for k in rating_info.keys():\n",
    "#     file.write(rating_info[k]['user_id'])\n",
    "#     file.write('::')\n",
    "#     file.write(rating_info[k]['book_id'])\n",
    "#     file.write('::')\n",
    "#     file.write(rating_info[k]['score'])\n",
    "#     file.write('\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 系统功能概述\n",
    "使用Python语言、PaddlePaddle深度学习核心框架、NUMPY、Embedding以及Linear等Python中核心扩展程序库对豆瓣图书数据集进行数据处理、推荐模型设计、模型训练、特征保存、最后利用需要进行推荐的用户特征或图书特征与保存的用户特征或图书特征构建相似度矩阵，通过对相似度矩阵排序得出相似度较高的内容对用户进行基于内容或协同过滤推荐，热门以及新品的图书推荐。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bf53915775404d03b9a325b12469b2b0a7b8ed26394e4166be318d1ffad41724\"></center>\n",
    "\n",
    "<center><br>图1：图书推荐系统的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据处理与读取模块\n",
    "数据处理，将豆瓣图书评分数据集的数据处理成神经网络理解的形式——使用Python语言和NUMPY库来构建神经网络模型。分别读取图书数据、用户数据以及评分数据，将图书数据、用户数据以及评分数据处理后存储到字典，通过评分数据字典构建一个带有数据迭代功能的数据读取器。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/fe66622d75c8490498c02e275bb1b3e2f3caffa63bd2409689f4e3c0e95e6a6c\"></center>\n",
    "\n",
    "<center><br>图2：数据处理与读取模块的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 用户数据处理\n",
    "用户原始数据文件userdata.dat中的数据格式为：用户ID::性别::年龄::职业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有用户数据： 81416 条\n",
      "第一条数据是： 1::男::58::公共卫生医师\n",
      "\n",
      "数据类型： <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 打开用户数据文件，读取所有用户数据到data中\n",
    "with open(user_info_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "# 打印data的数据长度、第一条数据、数据类型\n",
    "print(\"共有用户数据：\",len(data), \"条\")\n",
    "print(\"第一条数据是：\", data[0])\n",
    "print(\"数据类型：\", type(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从输出的内容可以看出，总共有81416条以字符串形式保存在用户数据文件的原始用户数据，每一行表示一个用户的数据，以::隔开，第一列到最后一列分别表示用户ID、用户性别、用户年龄、用户职业，就如第一条数据“1：：男：：58:：：公共卫生医师”就表示用户ID为1的用户是以为58岁的男性公共卫生医师。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "用户的性别男、女是字符串类型，需要通过代码转化为整型或浮点型数据表示，转换过程如下：\n",
    "```\n",
    "if gender == '男':\n",
    "\tgender = 1\n",
    "else:\n",
    "\tgender = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "用户的职业是字符串类型，需要通过代码转化为整型或浮点型数据表示，这里需要创建一个用户职业字典jobs，保存所有的用户职业，再将用户的职业数据替换成数字表示，转换过程如下：\n",
    "```\n",
    "if job not in jobs:\n",
    "\tjob_count += 1\n",
    "\tjobs [job] = job_count\n",
    "    \n",
    "job = jobs[job]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来把用户数据的字符串类型的数据转成数字类型，并存储到字典中，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 打开文件，编码方式选择UTF-8-sig，读取所有数据到data中\n",
    "with open(user_info_path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "user_info = {} # 用户信息字典\n",
    "jobs = {} # 用户职业字典\n",
    "job_count = 0 # 用户职业数量\n",
    "max_user_age = 0 # 最大用户年龄\n",
    "\n",
    "for item in data:\n",
    "    item = item.strip().split(\"::\")\n",
    "    \n",
    "    user_id = item[0]\n",
    "    gender = item[1]\n",
    "    if gender == '男':\n",
    "        gender = 1\n",
    "    else:\n",
    "        gender = 0\n",
    "    age = item[2]\n",
    "    job = item[3]\n",
    "\n",
    "    if job not in jobs:\n",
    "        job_count += 1\n",
    "        jobs [job] = job_count\n",
    "    \n",
    "    job = jobs[job]\n",
    "    \n",
    "    user_info[str(user_id)] = {'user_id': int(user_id), 'gender': int(gender), 'age': int(age), 'job': job}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "处理后的用户数据如下：\n",
    "\n",
    "`用户ID为1用户数据是： {'user_id': 1, 'gender': 1, 'age': 58, 'job': 1}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 图书数据处理\n",
    "图书数据文件bookdata.dat中的数据格式为：图书ID::图书标题::图书类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有图书数据： 70704 条\n",
      "第一条数据是： 1::政治无意识::社会学 政治学 文化研究 文学理论 \n",
      "\n",
      "数据类型： <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 打开图书数据文件，读取所有图书数据到data中\n",
    "with open(book_info_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "# 打印data的数据长度、第一条数据、数据类型\n",
    "print(\"共有图书数据：\",len(data), \"条\")\n",
    "print(\"第一条数据是：\", data[0])\n",
    "print(\"数据类型：\", type(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从输出的内容可以看出，总共有70704条以字符串形式保存在图书数据文件的原始图书数据，每一行表示一个图书的数据，以::隔开，第一列到最后一列分别表示图书ID、图书标题、图书类别，就如第一条数据“1：：政治无意识：：社会学 政治学 文化研究 文学理论”就表示图书ID为1的图书是一本标题为政治无意识讨论社会学、政治学、文化研究、文学理论的图书。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图书的类别是字符串数据，这里需要转换成数字表示，这里需要创建一个图书字典book_cats，保存所有的图书类别，再将图书的类别数据替换成数字表示，转换过程如下：\n",
    "```\n",
    "for cat in cats:\n",
    "    if cat not in book_cats:\n",
    "        cat_count += 1\n",
    "        book_cats[cat] = cat_count\n",
    "\n",
    "book_cat = [int(book_cats[k]) for k in cats]\n",
    "book_cat.sort()\n",
    "book_cat = book_cat[:cat_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来把图书数据的字符串类型的数据转成数字类型，并存储到字典中，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 打开文件，编码方式选择utf-8，读取所有数据到data中 \n",
    "with open(book_info_path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# 统计图书ID信息、图书标题、图书类别，并给每个类别一个数字序号。\n",
    "book_info = {} # 图书信息字典\n",
    "book_cats = {} # 图书类别字典\n",
    "cat_count = 0 # 图书类比数量\n",
    "book_count = 0 # 图书数量\n",
    "# 遍历原始数据\n",
    "for item in data:\n",
    "    item = item.strip().split(\"::\")\n",
    "    \n",
    "    # 定义图书ID\n",
    "    book_id = item[0]\n",
    "    # 定义图书标题\n",
    "    book_title = item[1]\n",
    "    # 获取图书类别\n",
    "    cats = item[2].split(' ')\n",
    "\n",
    "    book_count += 1\n",
    "    book_id = book_count\n",
    "\n",
    "    # 统计图书类别单词，并给每个单词一个序号，放在book_cats中\n",
    "    for cat in cats:\n",
    "        if cat not in book_cats:\n",
    "            cat_count += 1\n",
    "            book_cats[cat] = cat_count\n",
    "\n",
    "    book_cat = [int(book_cats[k]) for k in cats]\n",
    "    book_cat.sort()\n",
    "    book_cat = book_cat[:cat_size]\n",
    "\n",
    "    # 补0使图书种类对应的列表长度为cat_size\n",
    "    while len(book_cat)<cat_size:\n",
    "            book_cat.append(0)\n",
    "\n",
    "    # 保存所有图书数据到字典中\n",
    "    book_info[str(book_id)] = {'book_id': int(book_id),'图书标题': book_title,'book_cat': book_cat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "处理后的图书数据如下：\n",
    "\n",
    "`图书ID为1图书数据是： {'book_id': 1, '图书标题': '政治无意识', 'book_cat': [1, 2, 3, 4]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  评分数据处理\n",
    "评分数据文件ratingdata.dat中的数据格式为：图书ID::图书标题::图书类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有评分数据： 2034806 条\n",
      "第一条数据是： 1::70206::3\n",
      "\n",
      "数据类型： <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 打开评分数据文件，读取所有评分数据到data中\n",
    "with open(rating_info_path, 'r') as f:\n",
    "    data = f.readlines()\n",
    "# 打印data的数据长度、第一条数据、数据类型\n",
    "print(\"共有评分数据：\",len(data), \"条\")\n",
    "print(\"第一条数据是：\", data[0])\n",
    "print(\"数据类型：\", type(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从输出的内容可以看出，总共有2034806条以字符串形式保存在评分数据文件的原始评分数据，每一行表示一个用户的数据，以::隔开，第一列到最后一列分别表示用户ID、图书ID、评分。就如第一条数据“1：：70206：：3”就表示用户ID为1的用户给图书ID为70206的图书评的分数是3分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来把评分数据的字符串类型的数据转成整型或浮点型类型，并存储到字典中，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取文件里的数据\n",
    "with open(rating_info_path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# 创建评分信息字典\n",
    "rating_info = {}\n",
    "max_rating_length = 0 # 用户最大已评分图书数量\n",
    "rating_count = 0;  # 评分信息的数量\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    item = item.strip().split(\"::\")\n",
    "\n",
    "    user_id = item[0]\n",
    "    book_id = item[1]\n",
    "    score = item[2]\n",
    "\n",
    "    if user_id not in rating_info.keys():\n",
    "        rating_info[user_id] = {book_id:float(score)}\n",
    "    else:\n",
    "        rating_info[user_id][book_id] = float(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "处理后的评分数据如下：\n",
    "\n",
    "`用户ID为1评分数据是： {'70206': 3.0, '61049': 5.0, '19398': 5.0, '15690': 5.0, '52444': 5.0, '16223': 5.0, '66286': 5.0, '69866': 5.0, '38947': 5.0, '66374': 3.0, '70163': 5.0, '66030': 5.0, '12277': 4.0, '19336': 5.0, '24281': 5.0, '24540': 5.0, '45036': 4.0, '43180': 5.0, '40702': 5.0, '37692': 5.0, '4798': 5.0}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建数据读取器\n",
    "数据读取器的构建是个性化推荐算法中承上启下的关键步骤，构建数据读取器的不仅仅能通过评分数据把用户和图书数据关联起来，共同储存在数据读取器中，更能使得训练模型阶段训练过程有序，快捷地进行。数据读取器函数的创建过程：首先，先创建一个列表trainset来储存读取器中的数据，再遍历储存在可变容器模型中的评分数据，把评分数据中的用户信息和图书信息按照字典中的数据补全到列表trainset中。函数结构如下图所示：\n",
    "```\n",
    "# 构建数据集\n",
    "def get_dataset(self, user_info, rating_info, book_info):\n",
    "    trainset = []\n",
    "    for user_id in rating_info.keys():\n",
    "        user_ratings = rating_info[user_id]\n",
    "        for book_id in user_ratings:\n",
    "            if user_ratings[book_id] == 5:\n",
    "                trainset.append({'user_info': user_info[user_id],\n",
    "                                'book_info': book_info[book_id],\n",
    "                                'scores': user_ratings[book_id]})\n",
    "    return trainset \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据处理Python类\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Linear, Embedding, Conv2D\n",
    "import paddle.nn.functional as F\n",
    "import math\n",
    "import pickle \n",
    "\n",
    "# 得到图书数据方法\n",
    "def get_book_info(path, state):\n",
    "    # 打开文件，编码方式选择utf-8，读取所有数据到data中 \n",
    "    with open(path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # 统计图书ID信息、图书标题、图书类别，并给每个类别一个数字序号。\n",
    "    book_info = {} # 图书信息字典\n",
    "    book_cats = {} # 图书类别字典\n",
    "    cat_count = 0 # 图书类比数量\n",
    "    book_count = 0 # 图书数量\n",
    "    max_book_cats = 0 # 最大图书类比\n",
    "    # 遍历原始数据\n",
    "    for item in data:\n",
    "        item = item.strip().split(\"::\")\n",
    "        \n",
    "        # 定义图书ID\n",
    "        book_id = item[0]\n",
    "        # 定义图书标题\n",
    "        book_title = item[1]\n",
    "        # 获取图书类别\n",
    "        cats = item[2].split(' ')\n",
    "\n",
    "        book_count += 1\n",
    "        book_id = book_count\n",
    "\n",
    "        # 统计图书类别单词，并给每个单词一个序号，放在book_cats中\n",
    "        for cat in cats:\n",
    "            if cat not in book_cats:\n",
    "                cat_count += 1\n",
    "                book_cats[cat] = cat_count\n",
    "\n",
    "        book_cat = [int(book_cats[k]) for k in cats]\n",
    "        book_cat.sort()\n",
    "        book_cat = book_cat[:cat_size]\n",
    "\n",
    "        # 补0使图书种类对应的列表长度为cat_size\n",
    "        while len(book_cat)<cat_size:\n",
    "                book_cat.append(0)\n",
    "\n",
    "        # 保存所有图书数据到字典中\n",
    "        book_info[str(book_id)] = {'book_id': int(book_id),'图书标题': book_title,'类别': cats}\n",
    "\n",
    "        if state:\n",
    "            book_info[str(book_id)]['book_cat'] = book_cat\n",
    "\n",
    "        max_book_cats = max(max_book_cats, list_max(book_cat))\n",
    "\n",
    "    return book_info, book_cats, max_book_cats\n",
    "\n",
    "# 得到用户数据\n",
    "def get_user_info(path, state):\n",
    "\n",
    "    # 打开文件，编码方式选择UTF-8-sig，读取所有数据到data中\n",
    "    with open(path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    user_info = {} # 用户信息字典\n",
    "    jobs = {} # 用户职业字典\n",
    "    job_count = 0 # 用户职业数量\n",
    "    max_user_age = 0 # 最大用户年龄\n",
    "\n",
    "    for item in data:\n",
    "        item = item.strip().split(\"::\")\n",
    "        \n",
    "        user_id = item[0]\n",
    "        gender = item[1]\n",
    "        if gender == '男':\n",
    "            gender = 1\n",
    "        else:\n",
    "            gender = 0\n",
    "        age = item[2]\n",
    "        job = item[3]\n",
    "\n",
    "        if job not in jobs:\n",
    "            job_count += 1\n",
    "            jobs [job] = job_count\n",
    "        \n",
    "        job = jobs[job]\n",
    "        \n",
    "        user_info[str(user_id)] = {'user_id': int(user_id), '性别': item[1], '年龄': item[2], '职业': item[3]}\n",
    "\n",
    "        if state:\n",
    "            user_info[str(user_id)]['gender'] = int(gender)\n",
    "            user_info[str(user_id)]['age'] = int(age)\n",
    "            user_info[str(user_id)]['job'] = job\n",
    "        \n",
    "        max_user_age = max(max_user_age, int(age))\n",
    "    return user_info, jobs, max_user_age\n",
    "\n",
    "# 得到评分数据\n",
    "def get_rating_info(path):\n",
    "    # 读取文件里的数据\n",
    "    with open(path, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    # 创建评分信息字典\n",
    "    rating_info = {}\n",
    "    max_rating_length = 0 # 用户最大已评分图书数量\n",
    "    rating_count = 0;  # 评分信息的数量\n",
    "\n",
    "\n",
    "    for item in data:\n",
    "        item = item.strip().split(\"::\")\n",
    "\n",
    "        user_id = item[0]\n",
    "        book_id = item[1]\n",
    "        score = item[2]\n",
    "\n",
    "        if user_id not in rating_info.keys():\n",
    "            rating_info[user_id] = {book_id:float(score)}\n",
    "        else:\n",
    "            rating_info[user_id][book_id] = float(score)\n",
    "    \n",
    "    return rating_info\n",
    "\n",
    "class DataProcessing(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.book_info, self.book_cats, self.max_book_cats = get_book_info(book_info_path, True)\n",
    "        # 记录图书的最大ID\n",
    "        self.max_book_id = np.max(list(map(int, self.book_info.keys()))) \n",
    "        \n",
    "        # 得到用户数据\n",
    "        self.user_info, self.user_jobs, self.max_user_age = get_user_info(user_info_path, True)\n",
    "\n",
    "        # 记录用户数据的最大ID\n",
    "        self.max_user_id = np.max(list(map(int, self.user_info.keys())))\n",
    "        self.max_user_jobs = len(self.user_jobs)        \n",
    "\n",
    "        # 得到评分数据\n",
    "        self.rating_info = get_rating_info(rating_info_path)\n",
    "\n",
    "        for user_id in self.rating_info.keys():\n",
    "            user_preferences = []\n",
    "            cats_list = {}\n",
    "\n",
    "            for book_id in self.rating_info[user_id] :\n",
    "                for cat in self.book_info[book_id]['book_cat'] :\n",
    "                    if cat == 0:\n",
    "                        continue\n",
    "                    user_preferences.append(cat)\n",
    "\n",
    "            if len(list(set(user_preferences))) < 4:\n",
    "                preferences = list(set(user_preferences))\n",
    "                while len(preferences) < 4:\n",
    "                    preferences.append(0)\n",
    "                self.user_info[user_id]['favorite_type'] = preferences\n",
    "                continue\n",
    "\n",
    "            preferences = []\n",
    "\n",
    "            for cat in user_preferences:\n",
    "                if cat not in cats_list.keys():\n",
    "                    cats_list[cat] = user_preferences.count(cat)\n",
    "\n",
    "            while len(preferences) < 4 :\n",
    "                max_num = 0\n",
    "                for cat in cats_list.keys():\n",
    "                    max_num = max(max_num,cats_list[cat])\n",
    "\n",
    "                del_cat = []\n",
    "                for cat in cats_list.keys():\n",
    "                    if cats_list[cat] == max_num :\n",
    "                        preferences.append(cat)\n",
    "                        del_cat.append(cat)\n",
    "\n",
    "                for cat in del_cat :\n",
    "                    cats_list.pop(cat)\n",
    "\n",
    "            preferences = preferences[:4]\n",
    "            preferences.sort()\n",
    "            while len(preferences) < 4:\n",
    "                preferences.append(0)\n",
    "            self.user_info[user_id]['favorite_type'] = preferences\n",
    "\n",
    "        # 构建数据集 \n",
    "        self.dataset = self.get_dataset(user_info=self.user_info,\n",
    "                                        rating_info=self.rating_info,\n",
    "                                        book_info=self.book_info)\n",
    "\n",
    "        # 划分数据集，获得数据加载器\n",
    "        self.train_dataset = self.dataset[:int(len(self.dataset)*0.8)] # 训练数据\n",
    "        self.valid_dataset = self.dataset[int(len(self.dataset)*0.8):] # 验证数据\n",
    "        # print(\"数据集实例总数: \", len(self.dataset))\n",
    "        # print(\"图书数据信息:  用户数量: {}  图书数量: {}\".format(len(self.user_info),len(self.book_info)))\n",
    "\n",
    "    # 构建数据集\n",
    "    def get_dataset(self, user_info, rating_info, book_info):\n",
    "        trainset = []\n",
    "        for user_id in rating_info.keys():\n",
    "            user_ratings = rating_info[user_id]\n",
    "            for book_id in user_ratings:\n",
    "                if user_ratings[book_id] == 5:\n",
    "                    trainset.append({'user_info': user_info[user_id],\n",
    "                                    'book_info': book_info[book_id],\n",
    "                                    'scores': user_ratings[book_id]})\n",
    "        return trainset    \n",
    "\n",
    "    # 加载数据\n",
    "    def load_data(self, dataset=None, mode='train'):\n",
    "\n",
    "        data_length = len(dataset)\n",
    "        index_list = list(range(data_length))\n",
    "\n",
    "        # 定义数据迭代加载器\n",
    "        def data_generator():\n",
    "            # 训练模式下，打乱训练数据\n",
    "            if mode == 'train':\n",
    "                random.shuffle(index_list)\n",
    "            # 声明每个特征的列表\n",
    "            user_id_list, user_gender_list, user_age_list, user_job_list, favorite_type_list = [], [], [], [], []\n",
    "            book_id_list,book_cat_list = [], []\n",
    "            score_list = []\n",
    "\n",
    "            # 索引遍历输入数据集\n",
    "            for idx, i in enumerate(index_list):\n",
    "                # 获得特征数据保存到对应特征列表中\n",
    "                user_id_list.append(dataset[i]['user_info']['user_id'])\n",
    "                user_gender_list.append(dataset[i]['user_info']['gender'])\n",
    "                user_age_list.append(dataset[i]['user_info']['age'])\n",
    "                user_job_list.append(dataset[i]['user_info']['job'])\n",
    "                favorite_type_list.append(dataset[i]['user_info']['favorite_type'])\n",
    "\n",
    "                book_id_list.append(dataset[i]['book_info']['book_id'])\n",
    "                book_cat_list.append(dataset[i]['book_info']['book_cat'])\n",
    "                book_id = dataset[i]['book_info']['book_id']\n",
    "\n",
    "                score_list.append(int(dataset[i]['scores']))\n",
    "\n",
    "                # 如果读取的数据量达到当前的batch大小，就返回当前批次\n",
    "                if len(user_id_list)==BATCHSIZE:\n",
    "                    # 转换列表数据为数组形式，reshape到固定形状\n",
    "                    user_id_arr = np.array(user_id_list)\n",
    "                    user_gender_arr = np.array(user_gender_list)\n",
    "                    user_age_arr = np.array(user_age_list)\n",
    "                    user_job_arr = np.array(user_job_list)\n",
    "                    favorite_type_arr = np.reshape(np.array(favorite_type_list), [BATCHSIZE, 4]).astype(np.int64)\n",
    "\n",
    "                    book_id_arr = np.array(book_id_list)\n",
    "                    book_cat_arr = np.reshape(np.array(book_cat_list), [BATCHSIZE, cat_size]).astype(np.int64)\n",
    "\n",
    "                    scores_arr = np.reshape(np.array(score_list), [-1, 1]).astype(np.float32)\n",
    "\n",
    "                    # 放回当前批次数据\n",
    "                    yield [user_id_arr, user_gender_arr, user_age_arr,user_job_arr, favorite_type_arr],[book_id_arr, book_cat_arr], scores_arr\n",
    "       \n",
    "                    # 清空数据\n",
    "                    user_id_list, user_gender_list, user_age_list, user_job_list, favorite_type_list= [], [], [], [], []\n",
    "                    book_id_list, book_cat_list = [], []\n",
    "                    score_list = []\n",
    "\n",
    "        return data_generator\n",
    "\n",
    "# # 声明数据读取类\n",
    "# dataset = DataProcessing()\n",
    "# # 定义数据读取器\n",
    "# train_loader = dataset.load_data(dataset=dataset.train_dataset, mode='train')\n",
    "# # 迭代的读取数据\n",
    "# for idx, data in enumerate(train_loader()):\n",
    "#     user, book, score = data\n",
    "#     print(\"打印图书ID，类别数据的维度：\")\n",
    "#     for v in book:\n",
    "#         print(v.shape)\n",
    "#     print(\"打印用户ID，用户性别、用户年龄、用户职业的维度：\")\n",
    "#     for v in user:\n",
    "#         print(v.shape)    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 推荐模型设计模块\n",
    "设计神经网络模型，将离散的用户或图书各个属性的字符串数据映射为向量表示——将用户的性别、年龄、职业等原始特征和图书的各个类别特征转变成特征向量的表示，再将用户的性别、年龄、职业的特征向量合并成一个用户特征向量，图书的各个类别的特征向量合并成一个图书特征向量，合并得到的两个特征向量会使得用户和图书的相似度计算变得更方便。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/430b8c28858447c6ba59a229ed65e75c3e24e7e2a7dc481e95afd65eb143735a\"></center>\n",
    "\n",
    "<center><br>图3：推荐模型模块的流程图</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 用户模型设计\n",
    "用户数据处理后的用户特征中有用户ID、用户性别、用户职业和用户年龄这四个属性信息，其中所有用户的ID大相径庭，提取用户ID的属性信息没有意义，使用提取用户特征只需要提取用户数据中用户性别、用户职业和用户年龄这三个属性信息。将用户的性别、年龄、职业等原始特征转变成特征向量的表示，再将用户的性别、年龄、职业的特征向量融合成一个用户特征向量。\n",
    "\n",
    "\n",
    "#### 1.提取用户性别特征\n",
    "因为用户性别数据较为简单，所以可以把把转换成TENSOR类型的用户性别数据根据最大性别数据（2）转换成16维的向量来表示用户性别数据特征。提取用户性别特征过程：\\\n",
    "1、定义最大用户性别数；\\\n",
    "2、定义用户性别嵌入层的嵌入字典的大小为最大性别数，每个嵌入向量的维度为16维；\\\n",
    "3、定义用户性别线性变换层的线性变换层输入单元的数目为16维，线性变换层输出单元的数目为16维；\\\n",
    "4、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "提取用户性别特征过程如下所示：\n",
    "```\n",
    "USER_GENDER_DICT_SIZE = 2\n",
    "user_gender_emb = Embedding(num_embeddings=USER_GENDER_DICT_SIZE, embedding_dim=16)\n",
    "user_gender_fc = Linear(in_features=16, out_features=16)\n",
    "\n",
    "# 1. 通过Embedding映射用户性别数据；\n",
    "user_gender = user_gender_emb(user_gender)\n",
    "# 2. 通过一个全连接层计算类别特征向量。\n",
    "user_gender = user_gender_fc(user_gender)\n",
    "user_gender = F.relu(user_gender)\n",
    "```\n",
    "提取用户性别特征结果如下所示：\n",
    "```\n",
    "输入的用户性别是: [1]\n",
    "用户性别特征的数据特征是： [[0.20610389 0.05126399 0.04882429 0.         0.         0.\n",
    "  0.56391144 0.         0.48957294 0.         0.14768249 0.\n",
    "  0.3547892  0.         0.11570093 0.00661527]] \n",
    "其形状是： [1, 16]\n",
    "```\n",
    "\n",
    "#### 2.提取用户年龄特征\n",
    "因为用户年龄数据较为简单，所以把转换成TENSOR类型的用户年龄数据根据最大年龄数据（60）转换成16维的向量来表示用户年龄数据特征。提取用户年龄特征过程：\n",
    "1、定义最大用户年龄数；\\\n",
    "2、定义用户年龄嵌入层的嵌入字典的大小为最大用户年龄数，每个嵌入向量的维度为16维；\\\n",
    "3、定义用户年龄线性变换层的线性变换层输入单元的数目为16维，线性变换层输出单元的数目为16维；\\\n",
    "4、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "提取用户年龄特征过程如下所示：\n",
    "```\n",
    "USER_AGE_DICT_SIZE = 60\n",
    "user_age_emb = Embedding(num_embeddings=USER_AGE_DICT_SIZE, embedding_dim=16)\n",
    "user_age_fc = Linear(in_features=16, out_features=16)\n",
    "\n",
    "# 1. 通过Embedding映射用户年龄数据；\n",
    "user_age = user_age_emb(user_age)\n",
    "# 2. 通过一个全连接层计算类别特征向量。\n",
    "user_age = user_age_fc(user_age)\n",
    "user_age = F.relu(user_age)\n",
    "```\n",
    "提取用户性别特征结果如下所示：\n",
    "```\n",
    "输入的用户年龄是: [18]\n",
    "用户年龄特征的数据特征是： [[0.         0.         0.14154392 0.         0.00146747 0.\n",
    "  0.         0.         0.         0.16632748 0.11296132 0.04608054\n",
    "  0.12454596 0.         0.         0.        ]] \n",
    "其形状是： [1, 16]\n",
    "```\n",
    "\n",
    "#### 3.提取用户职业特征\n",
    "因为用户职业数据较为简单，所以把转换成TENSOR类型的用户职业数据根据最大职业数据（167）转换成16维的向量来表示用户职业数据特征。提取用户职业特征过程：\n",
    "1、定义最大用户职业数；\\\n",
    "2、定义用户职业嵌入层的嵌入字典的大小为最大用户职业数，每个嵌入向量的维度为16维；\\\n",
    "3、定义用户职业线性变换层的线性变换层输入单元的数目为16维，线性变换层输出单元的数目为16维；\\\n",
    "4、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "提取用户职业特征过程如下所示：\n",
    "```\n",
    "USER_JOB_DICT_SIZE =167\n",
    "user_job_emb = Embedding(num_embeddings=USER_JOB_DICT_SIZE, embedding_dim=16)\n",
    "user_job_fc = Linear(in_features=16, out_features=16)\n",
    "\n",
    "# 1. 通过Embedding映射用户职业数据；\n",
    "user_job = user_job_emb(user_job)\n",
    "# 2. 通过一个全连接层计算类别特征向量。\n",
    "user_job = user_job_fc(user_job)\n",
    "user_job = F.relu(user_job)\n",
    "```\n",
    "提取用户性别特征结果如下所示：\n",
    "```\n",
    "输入的用户职业是: [99]\n",
    "用户年龄特征的数据特征是： [[0.         0.         0.         0.03703775 0.03236652 0.\n",
    "  0.         0.         0.         0.         0.01341185 0.\n",
    "  0.04149527 0.         0.         0.        ]] \n",
    "其形状是： [1, 16]\n",
    "```\n",
    "\n",
    "#### 4.提取用户喜好类型特征\n",
    "构建用户喜好类型的特征提取网络，使用Embedding层和全连接层提取用户喜好类型特征。因为用户喜好类型数据较为复杂，所以把转换成TENSOR类型的用户喜好类型数据根据最大用户喜好类型数据（8223）转换成32维的向量来表示用户喜好类型数据特征。提取用户喜好类型特征过程：\n",
    "1、定义最大用户喜好类型数；\\\n",
    "2、定义用户喜好类型嵌入层的嵌入字典的大小为最大用户喜好类型，每个嵌入向量的维度为32维；\\\n",
    "3、对向量沿着类别数量维度进行求和；\\\n",
    "4、定义用户喜好类型线性变换层的线性变换层输入单元的数目为32维，线性变换层输出单元的数目为32维；\\\n",
    "5、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "提取用户喜好类型特征过程如下所示：\n",
    "```\n",
    "BOOK_CAT_DICT_SIZE = 8223\n",
    "user_favorite_type_emb = Embedding(num_embeddings=BOOK_CAT_DICT_SIZE, embedding_dim=32,sparse=False)\n",
    "user_favorite_type_fc = Linear(in_features = 32, out_features = 32)\n",
    "\n",
    "# 1. 通过Embedding映射用户喜好类型数据；\n",
    "user_favorite_type = user_favorite_type_emb(user_favorite_type)\n",
    "# 2. 对Embedding后的向量沿着类别数量维度进行求和，得到一个类别映射向量；\n",
    "user_favorite_type = paddle.sum(user_favorite_type, axis=1, keepdim=False)\n",
    "# 3. 通过一个全连接层计算类别特征向量。\n",
    "user_favorite_type = user_favorite_type_fc(user_favorite_type)\n",
    "user_favorite_type = F.relu(user_favorite_type)\n",
    "```\n",
    "提取用户喜爱类别特征结果如下所示：\n",
    "```\n",
    "输入的用户喜爱类别是: [[1 2 3 4]]\n",
    "计算的用户喜爱类别的特征是 [[0.         0.         0.07065833 0.05659996 0.         0.05466485\n",
    "  0.72442687 0.5506426  0.41612685 0.         0.         0.2779635\n",
    "  0.11453706 0.         0.         0.         0.04539035 0.87724763\n",
    "  0.14470053 0.10192098 0.04368107 0.         0.         0.6003144\n",
    "  0.07802389 0.13062555 0.6158916  0.         0.         0.02123808\n",
    "  0.         0.55285466]] \n",
    "其形状是： [1, 32]\n",
    "```\n",
    "\n",
    "#### 5.融合用户特征\n",
    "将用户的性别、年龄、职业等原始特征转变成特征向量的表示，再将用户的性别、年龄、职业、用户喜爱类别的特征向量融合成一个用户特征向量。融合用户特征过程：\n",
    "1、定义用户特征线性变换层的线性变换层输入单元的数目为80维，线性变换层输出单元的数目为200维；\\ \n",
    "2、调用CONCAT函数整合用户的性别、职业、年龄、用户喜爱类别特征；\\\n",
    "3、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "融合用户特征过程如下所示：\n",
    "```\n",
    "user_combined = Linear(in_features=80, out_features=200)\n",
    "\n",
    "# 收集所有的用户特征\n",
    "_features = [  user_gender_feat,user_age_feat,user_job_feat, user_favorite_type_feat]\n",
    "\n",
    "print(\"用户性别、年龄、职业、喜爱类别特征的维度：\", [f.shape for f in _features])\n",
    "\n",
    "_features = [k.numpy() for k in _features]\n",
    "_features = [paddle.to_tensor(k) for k in _features]\n",
    "\n",
    "# 对特征沿着最后一个维度级联\n",
    "user_feat = paddle.concat(_features, axis=1)\n",
    "user_feat = F.tanh(user_combined(user_feat))\n",
    "print(\"用户融合后特征的维度是：\", user_feat.shape)    \n",
    "```\n",
    "融合用户特征结果如下所示：\n",
    "```\n",
    "用户性别、年龄、职业、喜爱类别特征的维度： [[1, 16], [1, 16], [1, 16], [1, 32]]\n",
    "用户融合后特征的维度是： [1, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 图书模型设计\n",
    "图书数据处理后的用户特征中有图书ID、图书标题、图书类别这三个属性信息，其中所有图书的ID和标题大相径庭，提取图书ID和标题的属性信息没有意义，使用提取图书特征只需要提取图书的类别这个属性信息。将图书的各个类别的原始特征转变成特征向量的表示，再将图书的各个类别的特征向量融合成一个图书特征向量。\n",
    "\n",
    "\n",
    "#### 1.提取图书类别特征\n",
    "构建图书类别的特征提取网络，使用Embedding层和全连接层提取图书类别特征。因为图书类别数据较为复杂，所以把转换成TENSOR类型的图书类别数据根据最大图书类别数据（8223）转换成32维的向量来表示图书类别数据特征。提取图书类别特征过程：\n",
    "1、定义最大图书类别数；\\\n",
    "2、定义图书类别嵌入层的嵌入字典的大小为最大图书类别数，每个嵌入向量的维度为32维；\\\n",
    "3、对向量沿着类别数量维度进行求和；\\\n",
    "4、定义图书类别线性变换层的线性变换层输入单元的数目为32维，线性变换层输出单元的数目为32维；\\\n",
    "5、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "提取图书类别特征过程如下所示：\n",
    "```\n",
    "BOOK_CAT_DICT_SIZE = 8223\n",
    "\n",
    "book_cat_emb = Embedding(num_embeddings=BOOK_CAT_DICT_SIZE, embedding_dim=32,sparse=False)\n",
    "book_cat_fc = Linear(in_features = 32, out_features = 32)\n",
    "\n",
    "# 1. 通过Embedding映射图书类别类型数据；\n",
    "book_cat_feat = book_cat_emb(book_cat)\n",
    "# 2. 对Embedding后的向量沿着类别数量维度进行求和，得到一个类别映射向量；\n",
    "book_cat_feat = paddle.sum(book_cat, axis=1, keepdim=False)\n",
    "# 3. 通过一个全连接层计算类别特征向量。\n",
    "book_cat_feat = book_cat_fc(book_cat_feat)\n",
    "book_cat_feat = F.relu(book_cat_feat)\n",
    "```\n",
    "提取图书类别特征结果如下所示：\n",
    "```\n",
    "输入的图书类别是: [[2 5 8 9]]\n",
    "计算的图书类别的特征是 [[0.         0.0168008  0.         0.00856885 0.03081331 0.03275225\n",
    "  0.0121304  0.         0.00151267 0.01032292 0.0135077  0.\n",
    "  0.         0.         0.         0.         0.         0.00016791\n",
    "  0.         0.02031635 0.05189621 0.0146278  0.01302264 0.\n",
    "  0.         0.         0.03033745 0.         0.07172493 0.02408044\n",
    "  0.         0.        ]] \n",
    "其形状是： [1, 32]\n",
    "```\n",
    "\n",
    "#### 2.融合图书特征\n",
    "将图书的各个类别的原始特征转变成特征向量的表示，再将图书的各个类别的特征向量融合成一个图书特征向量。融合图书特征过程：\n",
    "1、定义图书特征线性变换层的线性变换层输入单元的数目为32维，线性变换层输出单元的数目为200维； \\\n",
    "2、调用CONCAT函数整合图书的各个类别特征；\\\n",
    "3、调用RULE函数把转换后的结果中的负数变换为0。\\\n",
    "融合图书特征过程如下所示：\n",
    "```\n",
    "book_combined = Linear(in_features=32, out_features=200)\n",
    "\n",
    "# 收集所有的图书特征\n",
    "_features = [book_cat_feat]\n",
    "\n",
    "print(\"图书类别的维度：\", [f.shape for f in _features])\n",
    "\n",
    "_features = [k.numpy() for k in _features]\n",
    "_features = [paddle.to_tensor(k) for k in _features]\n",
    "\n",
    "# 对特征沿着最后一个维度级联\n",
    "book_feat = paddle.concat(_features, axis=1)\n",
    "book_feat = F.tanh(book_combined(book_feat))\n",
    "print(\"用户融合后特征的维度是：\", book_feat.shape)      \n",
    "```\n",
    "融合图书特征结果如下所示：\n",
    "```\n",
    "图书类别的维度： [[1, 32]]\n",
    "用户融合后特征的维度是： [1, 200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 相似度计算\n",
    "计算通过用户训练模型得到的用户特征和通过图书训练模型得到的图书特征的相似度。用户对该图书评了很高的分书则表示该用户对该图书的兴趣或喜爱程度很高，那么该用户和图书特征之间的相似度就会比其他图书特征要高。使用余弦相似度计算方式，用户特征和图书特征之间的相似度就可以通过计算两个向量之间的夹角余弦值得到。\n",
    "\n",
    "余弦相似度是一种比其他相似度计算方式更加简洁和更易于使用的向量相似度计算方式。\n",
    "\n",
    "余弦相似度的公式：\n",
    "$similarity = cos(\\theta) = \\frac{A\\cdot B}{A + B} = \\frac{\\sum_{i}^{n}A_i \\times B_i}{\\sqrt{\\sum_{i}^{n}(A_i)^2 + \\sum_{i}^{n}(B_i)^2}}$\n",
    "\n",
    "\n",
    "PaddlePaddle支持余弦相似度计算——cosine_similarity方法用于计算两个Tensor数据沿着某一维度的余弦相似度。\n",
    "cosine_similarity方法具体用法如下：\n",
    "```\n",
    "# 根据计算的特征计算相似度\n",
    "sim = F.common.cosine_similarity(user_features, book_features).reshape([-1, 1])\n",
    "# 将相似度扩大范围到和图书评分相同数据范围\n",
    "res = paddle.scale(sim, scale=5)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 推荐模型设计完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图书模型类\n",
    "\n",
    "class Model(paddle.nn.Layer):\n",
    "    def __init__(self, user_favorite_type, user_book_cat, user_gender_age_job,fc_sizes):\n",
    "        super(Model, self).__init__()\n",
    "                \n",
    "        # 将传入的name信息和bool型参数添加到模型类中\n",
    "        self.user_favorite_type = user_favorite_type\n",
    "        self.user_gender_age_job = user_gender_age_job\n",
    "        self.user_book_cat = user_book_cat\n",
    "        self.fc_sizes = fc_sizes\n",
    "        \n",
    "        # 获取数据集的信息，并构建训练和验证集的数据迭代器\n",
    "        Dataset = DataProcessing()\n",
    "        self.Dataset = Dataset\n",
    "        self.trainset = self.Dataset.train_dataset\n",
    "        self.valset = self.Dataset.valid_dataset\n",
    "        self.train_loader = self.Dataset.load_data(dataset=self.trainset, mode='train')\n",
    "        self.valid_loader = self.Dataset.load_data(dataset=self.valset, mode='valid')\n",
    "\n",
    "        \"\"\" 将用户信息定义成网络层 \"\"\"\n",
    "        USER_DICT_SIZE = Dataset.max_user_id + 1\n",
    "        # 对用户ID做映射，并紧接着一个FC层\n",
    "        self.user_emb = Embedding(num_embeddings=USER_DICT_SIZE,embedding_dim=32,sparse=False)\n",
    "        self.user_fc = Linear(in_features = 32, out_features = 32)\n",
    "        \n",
    "        # 对用户性别信息做映射，并紧接着一个Linear层\n",
    "        USER_GENDER_DICT_SIZE = 2\n",
    "        self.user_gender_emb = Embedding(num_embeddings=USER_GENDER_DICT_SIZE, embedding_dim=16)\n",
    "        self.user_gender_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 对用户年龄信息做映射，并紧接着一个Linear层\n",
    "        USER_AGE_DICT_SIZE = Dataset.max_user_age + 1\n",
    "        self.user_age_emb = Embedding(num_embeddings=USER_AGE_DICT_SIZE, embedding_dim=16)\n",
    "        self.user_age_fc = Linear(in_features=16, out_features=16)\n",
    "        \n",
    "        # 对用户职业信息做映射，并紧接着一个Linear层\n",
    "        USER_JOB_DICT_SIZE = Dataset.max_user_jobs + 1\n",
    "        self.user_job_emb = Embedding(num_embeddings=USER_JOB_DICT_SIZE, embedding_dim=16)\n",
    "        self.user_job_fc = Linear(in_features=16, out_features=16)\n",
    "\n",
    "        # 用户喜爱类别做映射\n",
    "        BOOK_CAT_DICT_SIZE = len(Dataset.book_cats) + 1\n",
    "\n",
    "        self.favorite_type_emb = Embedding(num_embeddings=BOOK_CAT_DICT_SIZE, embedding_dim=32,sparse=False)\n",
    "        self.favorite_type_fc = Linear(in_features = 32, out_features = 32)\n",
    "\n",
    "        if self.user_gender_age_job:\n",
    "            self.user_combined = Linear(in_features=48, out_features=200)\n",
    "\n",
    "        if self.user_favorite_type:\n",
    "            self.user_combined = Linear(in_features=32, out_features=200)\n",
    "\n",
    "        \"\"\" 将图书信息定义成网络层 \"\"\"\n",
    "        # 对图书ID信息做映射，并紧接着一个Linear层\n",
    "        BOOK_DICT_SIZE = Dataset.max_book_id + 1\n",
    "        self.book_emb = Embedding(num_embeddings=BOOK_DICT_SIZE, embedding_dim=32)\n",
    "        self.book_fc = Linear(in_features = 32, out_features = 32)\n",
    "\n",
    "        # 对图书类别做映射\n",
    "        BOOK_CAT_DICT_SIZE = len(Dataset.book_cats) + 1\n",
    "\n",
    "        self.book_cat_emb = Embedding(num_embeddings=BOOK_CAT_DICT_SIZE, embedding_dim=32,sparse=False)\n",
    "        self.book_cat_fc = Linear(in_features = 32, out_features = 32)\n",
    "\n",
    "        # 新建一个Linear层，用于整合图书特征\n",
    "        self.book_concat_embed = Linear(in_features =  32, out_features = 200)\n",
    "        # self.book_concat_embed = Linear(in_features =  32 * cat_size, out_features = 200)\n",
    "\n",
    "        user_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._user_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = paddle.nn.Linear(\n",
    "                in_features=user_sizes[i],\n",
    "                out_features=user_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=paddle.nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(user_sizes[i]))))\n",
    "            self.add_sublayer('linear_user_%d' % i, linear)\n",
    "            self._user_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = paddle.nn.ReLU()\n",
    "                self.add_sublayer('user_act_%d' % i, act)\n",
    "                self._user_layers.append(act)\n",
    "        \n",
    "        book_sizes = [200] + self.fc_sizes\n",
    "        acts = [\"relu\" for _ in range(len(self.fc_sizes))]\n",
    "        self._book_layers = []\n",
    "        for i in range(len(self.fc_sizes)):\n",
    "            linear = paddle.nn.Linear(\n",
    "                in_features=book_sizes[i],\n",
    "                out_features=book_sizes[i + 1],\n",
    "                weight_attr=paddle.ParamAttr(\n",
    "                    initializer=paddle.nn.initializer.Normal(\n",
    "                        std=1.0 / math.sqrt(book_sizes[i]))))\n",
    "            self.add_sublayer('linear_book_%d' % i, linear)\n",
    "            self._book_layers.append(linear)\n",
    "            if acts[i] == 'relu':\n",
    "                act = paddle.nn.ReLU()\n",
    "                self.add_sublayer('book_act_%d' % i, act)\n",
    "                self._book_layers.append(act)\n",
    "    \n",
    "    # 定义计算用户特征的前向运算过程\n",
    "    def get_user_feat(self, user_var):\n",
    "        \"\"\" 获取用户特征\"\"\"\n",
    "        # 获取到用户数据\n",
    "        user_id, user_gender, user_age, user_job, favorite_type = user_var\n",
    "        # 将用户的ID数据经过embedding和FC计算，得到的特征保存在feats_collect中\n",
    "        feats_collect = []\n",
    "        user_id = self.user_emb(user_id)\n",
    "        user_id = self.user_fc(user_id)\n",
    "        user_id = F.relu(user_id)\n",
    "        # feats_collect.append(user_id)\n",
    "\n",
    "        # 如果使用用户的喜爱种类数据，计算用户喜爱种类特征的映射\n",
    "        if self.user_favorite_type:\n",
    "            # 计算图书种类的特征映射，对多个种类的特征求和得到最终特征\n",
    "            favorite_type = self.favorite_type_emb(favorite_type)\n",
    "            favorite_type = paddle.sum(favorite_type, axis=1, keepdim=False)\n",
    "\n",
    "            favorite_type = self.favorite_type_fc(favorite_type)\n",
    "            feats_collect.append(favorite_type)\n",
    "\n",
    "        # 选择是否使用用户已评分图书特征\n",
    "        if self.user_gender_age_job:\n",
    "            # 计算用户的性别特征，并保存在feats_collect中\n",
    "            user_gender = self.user_gender_emb(user_gender)\n",
    "            user_gender = self.user_gender_fc(user_gender)\n",
    "            user_gender = F.relu(user_gender)\n",
    "            feats_collect.append(user_gender)\n",
    "            # 计算用户的年龄特征，并保存在feats_collect中\n",
    "            user_age = self.user_age_emb(user_age)\n",
    "            user_age = self.user_age_fc(user_age)\n",
    "            user_age = F.relu(user_age)\n",
    "            feats_collect.append(user_age)\n",
    "            # 计算用户的职业特征，并保存在feats_collect中\n",
    "            user_job = self.user_job_emb(user_job)\n",
    "            user_job = self.user_job_fc(user_job)\n",
    "            user_job = F.relu(user_job)\n",
    "            feats_collect.append(user_job)\n",
    "            \n",
    "        # 使用一个全连接层，整合所有电影特征，映射为一个200维的特征向量\n",
    "        user_feat = paddle.concat(feats_collect, axis=1)\n",
    "        user_features = F.tanh(self.user_combined(user_feat))\n",
    "\n",
    "        for n_layer in self._user_layers:\n",
    "            user_features = n_layer(user_features)\n",
    "        \n",
    "        return user_features\n",
    "\n",
    "    # 定义图书特征的前向计算过程\n",
    "    def get_book_feat(self,book_var):\n",
    "        \"\"\" 获取图书特征\"\"\"\n",
    "        # 获得图书数据\n",
    "        book_id, book_cat = book_var\n",
    "        feats_collect = []\n",
    "\n",
    "        # 计算图书ID的特征，并存在feats_collect中\n",
    "        book_id = self.book_emb(book_id)\n",
    "        book_id = self.book_fc(book_id)\n",
    "        book_id = F.relu(book_id)\n",
    "        # feats_collect.append(book_id)\n",
    "\n",
    "        # 如果使用图书的种类数据，计算图书种类特征的映射\n",
    "        if self.user_book_cat:\n",
    "            # 计算图书种类的特征映射，对多个种类的特征求和得到最终特征\n",
    "          \n",
    "            book_cat = self.book_cat_emb(book_cat)\n",
    "            book_cat = paddle.sum(book_cat, axis=1, keepdim=False)\n",
    "\n",
    "            book_cat = self.book_cat_fc(book_cat)\n",
    "            feats_collect.append(book_cat)\n",
    "                \n",
    "        # 使用一个全连接层，整合所有图书特征，映射为一个200维的特征向量\n",
    "        book_feat = paddle.concat(feats_collect, axis=1)\n",
    "        book_features = F.tanh(self.book_concat_embed(book_feat))\n",
    "        for n_layer in self._book_layers:\n",
    "            book_features = n_layer(book_features)\n",
    "        return book_features        \n",
    "    \n",
    "    # 定义个性化推荐算法的前向计算\n",
    "    def forward(self, user_var, book_var):\n",
    "        # 计算用户特征和图书特征\n",
    "        user_features = self.get_user_feat(user_var)\n",
    "        book_features = self.get_book_feat(book_var)\n",
    "       \n",
    "        # 根据计算的特征计算相似度\n",
    "        sim = F.common.cosine_similarity(user_features, book_features).reshape([-1, 1])\n",
    "        #使用余弦相似度算子，计算用户和图书的相似程度\n",
    "        # sim = F.cosine_similarity(user_features, book_features, axis=1).reshape([-1, 1])\n",
    "        # 将相似度扩大范围到和图书评分相同数据范围\n",
    "        res = paddle.scale(sim, scale=5)\n",
    "        return user_features, book_features, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型训练与特征保存模块\n",
    "配置训练参数并完成训练——用余弦相似度计算方式得出的用户和图书的向量之间的余弦相似度与训练样本（用户对于该图书的真实评分）的均方差做回归模型的损失计算。使用Pickle库的dump函数以二进制的方式把用户特征向量和图书特征向量参照训练好的模型参数以字典的形式保存到文件中，以方便后续实现图书推荐模块时直接加载。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d4a3844e73154217a174aeb308f9e7901efc2e25f5a045bc94a6d99f3cf4a196\"></center>\n",
    "<center><br>图4：模型训练模块的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存书籍特征数量： 70704\n",
      "已保存用户特征数量： 51416\n",
      "特征保存已完成\n"
     ]
    }
   ],
   "source": [
    "# 保存初始特征\n",
    "\n",
    "# 原始数据处理\n",
    "book_info , _, _ = get_book_info(book_info_path, True)\n",
    "user_info , _, _ = get_user_info(user_info_path, True)\n",
    "# 得到评分数据\n",
    "rating_info = get_rating_info(rating_info_path)\n",
    "\n",
    "for user_id in rating_info.keys():\n",
    "        user_info[user_id]['favorite_type'] = [0,0,0,0]\n",
    "\n",
    "# 加载第三方库Pickle，用来保存Python数据到本地\n",
    "import pickle\n",
    "# 定义特征保存函数\n",
    "def get_user_book_features(model):\n",
    "    paddle.set_device('cpu') \n",
    "    user_pkl = {}\n",
    "    book_pkl = {}\n",
    "    \n",
    "    # 定义将list中每个元素转成tensor的函数\n",
    "    def list2tensor(inputs, shape):\n",
    "        inputs = np.reshape(np.array(inputs).astype(np.int64), shape)\n",
    "        return paddle.to_tensor(inputs)\n",
    "\n",
    "    for i in user_info.keys():\n",
    "\n",
    "        # 获得用户数据，计算得到用户特征，保存在user_pkl字典中\n",
    "        if i not in user_pkl.keys():\n",
    "\n",
    "            user_id_v = list2tensor(user_info[i]['user_id'], [1])\n",
    "            user_gender_v = list2tensor(user_info[i]['gender'], [1])\n",
    "            user_age_v = list2tensor(user_info[i]['age'], [1])\n",
    "            user_job_v = list2tensor(user_info[i]['job'], [1])\n",
    "            favorite_type_v = list2tensor(user_info[i]['favorite_type'], [1, 4])\n",
    "\n",
    "            user_in = [user_id_v, user_gender_v,user_age_v, user_job_v, favorite_type_v]\n",
    "            user_feat = model.get_user_feat(user_in)\n",
    "\n",
    "            user_pkl[i] = user_feat.numpy()\n",
    "    \n",
    "    for i in book_info.keys():\n",
    "        \n",
    "        # 获得图书数据，计算得到图书特征，保存在book_pkl字典中\n",
    "        if i not in book_pkl.keys():\n",
    "            book_id_v = list2tensor(book_info[i]['book_id'], [1])\n",
    "            book_cat_v = list2tensor(book_info[i]['book_cat'], [1, cat_size])\n",
    "            # book_cat_v = np.reshape(np.array(book_info[i]['book_cat']).astype(np.int64), [1, cat_size])\n",
    "\n",
    "            book_in = [book_id_v, book_cat_v]\n",
    "            book_feat = model.get_book_feat(book_in)\n",
    "\n",
    "            book_pkl[i] = book_feat.numpy()\n",
    "    \n",
    "    print(\"已保存书籍特征数量：\",len(book_pkl.keys()))    \n",
    "    print(\"已保存用户特征数量：\",len(user_pkl.keys()))\n",
    "    # 保存特征到本地\n",
    "    pickle.dump(user_pkl, open(user_raw_feat, 'wb'))\n",
    "    pickle.dump(book_pkl, open(book_raw_feat, 'wb'))\n",
    "    print(\"特征保存已完成\")\n",
    "\n",
    "fc_sizes=[128, 64, 32]\n",
    "user_favorite_type, user_book_cat, user_gender_age_job =False, True, True\n",
    "model = Model(user_favorite_type, user_book_cat, user_gender_age_job,fc_sizes)\n",
    "\n",
    "get_user_book_features(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练(均方差损失函数) \n",
    "\n",
    "#### 1.训练配置\n",
    "\n",
    "声明定义好的回归模型实例为Model，并将模型的状态设置为train；\\\n",
    "使用load_data函数加载训练数据和测试数据；\\\n",
    "使用adam优化器，设置优化算法和学习率，优化算法采用随机梯度下降SGD，学习率设置为0.01。\n",
    "\n",
    "\n",
    "训练过程采用二层循环嵌套方式：\n",
    "* 内层循环： 负责整个数据集的一次遍历，采用分批次方式（batch）。数据集样本数量为1581886，一个批次有256样本，则遍历一次数据集的批次数量是1581886/256=6180，即内层循环需要执行6180次。\n",
    "* 外层循环： 定义遍历数据集的次数，通过参数Epoches设置。\n",
    "\n",
    "batch的取值会影响模型训练效果，batch过大，会增大内存消耗和计算时间，且训练效果并不会明显提升（每次参数只向梯度反方向移动一小步，因此方向没必要特别精确）；batch过小，每个batch的样本数据没有统计意义，计算的梯度方向可能偏差较大。\n",
    "```\n",
    "lr = 0.001\n",
    "Epoches = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 获得数据读取器\n",
    "data_loader = model.train_loader\n",
    "\n",
    "# 使用adam优化器，设置优化算法和学习率，优化算法采用随机梯度下降SGD，学习率设置为0.01。\n",
    "opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "```\n",
    "#### 2.训练过程\n",
    "* **前向计算：** 将一个批次的样本数据灌入网络中，计算图书特征与用户特征的余弦相似度结果；\n",
    "```\n",
    "# 计算用户特征和图书特征\n",
    "user_features = self.get_user_feat(user_var)\n",
    "book_features = self.get_book_feat(book_var)\n",
    "\n",
    "# 根据计算的特征计算相似度\n",
    "sim = F.common.cosine_similarity(user_features, book_features).reshape([-1, 1])\n",
    "# 将相似度扩大范围到和图书评分相同数据范围\n",
    "res = paddle.scale(sim, scale=5)\n",
    "```\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。\n",
    "\n",
    "通过模型计算表示用户特征与图书特征之间的余弦相似度是$z$, 但实际评分数据是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型预测值和真实值差距的指标。\n",
    "\n",
    "\n",
    "* **计算损失函数：** 以前向计算结果和真实评分作为输入，通过损失函数square_error_cost API计算出损失函数值（Loss）。\n",
    "```\n",
    "# 计算loss\n",
    "loss = F.square_error_cost(scores_predict, scores_label)\n",
    "avg_loss = paddle.mean(loss)\n",
    "```\n",
    "\n",
    "* **反向传播：** 执行梯度反向传播backward函数，即从后到前逐层计算每一层的梯度，并根据设置的优化算法更新参数(opt.step函数)。\n",
    "```\n",
    "# 计算给定的 Tensors 的反向梯度。\n",
    "avg_loss.backward()\n",
    "# 执行一次优化器并进行参数更新。\n",
    "opt.step()\n",
    "# 清除需要优化的参数的梯度。\n",
    "opt.clear_grad()\n",
    "```\n",
    "\n",
    "#### 3.保存特征\n",
    "使用paddle.save将模型当前的参数数据 model.state_dict() 保存到文件中，用于模型预测或校验的程序调用。\n",
    "```\n",
    "# 每个epoch 保存一次模型\n",
    "paddle.save(model.state_dict(), './checkpoint/epoch'+str(epoch)+'.pdparams')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "# 模型训练(均方差损失函数)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model):\n",
    "    # 配置训练参数\n",
    "    lr = 0.001\n",
    "    Epoches = 10\n",
    "    paddle.set_device('cpu') \n",
    "\n",
    "    # 启动训练\n",
    "    model.train()\n",
    "    # 获得数据读取器\n",
    "    data_loader = model.train_loader\n",
    "    # 使用adam优化器，学习率使用0.01\n",
    "    opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(0, Epoches):\n",
    "        for idx, data in enumerate(data_loader()):\n",
    "            # 获得数据，并转为tensor格式\n",
    "            user, book, score = data\n",
    "            user_v = [paddle.to_tensor(var) for var in user]\n",
    "            book_v = [paddle.to_tensor(var) for var in book]\n",
    "            scores_label = paddle.to_tensor(score)\n",
    "            # 计算出算法的前向计算结果\n",
    "            _, _, scores_predict = model(user_v, book_v)\n",
    "            # 计算loss\n",
    "            loss = F.square_error_cost(scores_predict, scores_label)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            if epoch != 0:\n",
    "                losses.append(avg_loss.numpy())\n",
    "\n",
    "            if idx % 2000 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, idx, avg_loss.numpy()))\n",
    "                \n",
    "            # 损失函数下降，并清除梯度\n",
    "            avg_loss.backward() # 计算给定的 Tensors 的反向梯度。\n",
    "            opt.step() # 执行一次优化器并进行参数更新。\n",
    "            opt.clear_grad() # 清除需要优化的参数的梯度。\n",
    "\n",
    "        # 每个epoch 保存一次模型\n",
    "        paddle.save(model.state_dict(), './checkpoint/epoch'+str(epoch)+'.pdparams')\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [8.186678]\n",
      "epoch: 0, batch_id: 2000, loss is: [8.383435e-06]\n",
      "epoch: 1, batch_id: 0, loss is: [5.323963e-06]\n",
      "epoch: 1, batch_id: 2000, loss is: [1.3618335e-06]\n",
      "epoch: 2, batch_id: 0, loss is: [1.0469047e-06]\n",
      "epoch: 2, batch_id: 2000, loss is: [4.6272092e-07]\n",
      "epoch: 3, batch_id: 0, loss is: [3.1155577e-07]\n",
      "epoch: 3, batch_id: 2000, loss is: [1.1430134e-07]\n",
      "epoch: 4, batch_id: 0, loss is: [9.1580304e-08]\n",
      "epoch: 4, batch_id: 2000, loss is: [3.5822936e-08]\n",
      "epoch: 5, batch_id: 0, loss is: [2.130345e-08]\n",
      "epoch: 5, batch_id: 2000, loss is: [9.504508e-09]\n",
      "epoch: 6, batch_id: 0, loss is: [8.127591e-09]\n",
      "epoch: 6, batch_id: 2000, loss is: [3.9182995e-09]\n",
      "epoch: 7, batch_id: 0, loss is: [1.9974982e-09]\n",
      "epoch: 7, batch_id: 2000, loss is: [8.8780894e-10]\n",
      "epoch: 8, batch_id: 0, loss is: [5.5398086e-10]\n",
      "epoch: 8, batch_id: 2000, loss is: [3.1595082e-10]\n",
      "epoch: 9, batch_id: 0, loss is: [3.941114e-10]\n",
      "epoch: 9, batch_id: 2000, loss is: [1.1056045e-10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(obj, collections.Iterator):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return list(data) if isinstance(data, collections.MappingView) else data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 启动训练\n",
    "fc_sizes=[128, 64, 32]\n",
    "user_favorite_type, user_book_cat, user_gender_age_job =True, True, False\n",
    "model = Model(user_favorite_type, user_book_cat, user_gender_age_job,fc_sizes)\n",
    "losses =train(model)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 特征保存\n",
    "保存训练的模型特征过程：\\\n",
    "1、创建用户特征字典和图书特征字典；\\\n",
    "2、遍历数据读取器里的数据提取出用户信息和图书信息；\\\n",
    "3、把需要转换的用户信息和图书信息转化为TENSOR类型；\\\n",
    "4、调用图书模型或用户模型把TENSO类型的图书和用户信息提取成向量特征；\\\n",
    "5、使用Pickle库的dump函数以二进制的方式把未经过训练的或经过训练的用户特征向量和未经过训练的或经过训练图书特征向量参照训练好的模型参数以字典的形式保存到文件中。\\\n",
    "保存特征的函数图下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 保存特征完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存书籍特征数量： 10687\n",
      "已保存用户特征数量： 51416\n",
      "特征保存已完成\n"
     ]
    }
   ],
   "source": [
    "# 保存训练特征\n",
    "\n",
    "# 加载第三方库Pickle，用来保存Python数据到本地\n",
    "import pickle\n",
    "# 定义特征保存函数\n",
    "def get_user_book_features(model, params_file_path):\n",
    "    paddle.set_device('cpu') \n",
    "    user_pkl = {}\n",
    "    book_pkl = {}\n",
    "    \n",
    "    # 定义将list中每个元素转成tensor的函数\n",
    "    def list2tensor(inputs, shape):\n",
    "        inputs = np.reshape(np.array(inputs).astype(np.int64), shape)\n",
    "        return paddle.to_tensor(inputs)\n",
    "\n",
    "    # 加载模型参数到模型中，设置为验证模式eval（）\n",
    "    model_state_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    # 获得整个数据集的数据\n",
    "    dataset = model.Dataset.dataset\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        # 获得用户数据，图书数据，评分数据  \n",
    "        # 本案例只转换所有在样本中出现过的user和book，实际中可以使用业务系统中的全量数据\n",
    "        user_info, book_info, score = dataset[i]['user_info'], dataset[i]['book_info'],dataset[i]['scores']\n",
    "        userid = str(user_info['user_id'])\n",
    "        bookid = str(book_info['book_id'])\n",
    "        \n",
    "        # 获得图书数据，计算得到图书特征，保存在book_pkl字典中\n",
    "        if bookid not in book_pkl.keys():\n",
    "            book_id_v = list2tensor(book_info['book_id'], [1])\n",
    "            book_cat_v = list2tensor(book_info['book_cat'], [1, cat_size])\n",
    "            # book_cat_v = np.reshape(np.array(book_info['book_cat']).astype(np.int64), [1, cat_size])\n",
    "\n",
    "            book_in = [book_id_v, book_cat_v]\n",
    "            book_feat = model.get_book_feat(book_in)\n",
    "\n",
    "            book_pkl[bookid] = book_feat.numpy()\n",
    "\n",
    "        # 获得用户数据，计算得到用户特征，保存在user_pkl字典中\n",
    "        if userid not in user_pkl.keys():\n",
    "            user_id_v = list2tensor(user_info['user_id'], [1])\n",
    "            user_gender_v = list2tensor(user_info['gender'], [1])\n",
    "            user_age_v = list2tensor(user_info['age'], [1])\n",
    "            user_job_v = list2tensor(user_info['job'], [1])            \n",
    "            favorite_type_v = list2tensor(user_info['favorite_type'], [1, 4])\n",
    "\n",
    "            user_in = [user_id_v, user_gender_v,user_age_v, user_job_v, favorite_type_v]\n",
    "            user_feat = model.get_user_feat(user_in)\n",
    "\n",
    "            user_pkl[userid] = user_feat.numpy()\n",
    "    \n",
    "    print(\"已保存书籍特征数量：\",len(book_pkl.keys()))    \n",
    "    print(\"已保存用户特征数量：\",len(user_pkl.keys()))\n",
    "    # 保存特征到本地\n",
    "    pickle.dump(user_pkl, open(user_train_feat, 'wb'))\n",
    "    pickle.dump(book_pkl, open(book_train_feat, 'wb'))\n",
    "    print(\"特征保存已完成\")\n",
    "\n",
    "param_path = \"checkpoint/epoch9.pdparams\"\n",
    "get_user_book_features(model, param_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 图书推荐实现模块\n",
    "利用保存的特征构建相似度矩阵完成推荐：计算用户特征和图书特征之间的余弦相似度。基于内容过滤的图书推荐会根据用户已经评了高分的图书来推荐图书，通过计算用户已评分图书特征和其他所有保存在文件中的图书特征之间的相似度，对所有计算得出的相似度进行排序，把相识度较高的几本图书作为推荐图书。基于协同过滤的图书推荐会根据相似用户或具有相似喜好用户的喜好图书进行推荐图书，通过计算需要图书推荐的用户特征和其他文件里保存的所有用户特征之间的相似度，对通过余弦相似度计算方式计算得出相似度进行排序，取相似度较大的几个用户所喜爱的书图作为推荐图书。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc56944f7284ff0b919df5d35d12976ddb835350877421a9049b716badb9235\"></center>\n",
    "<center><br>图5：图书推荐实现模块的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 图书推荐实现完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图书推荐方法\n",
    "\n",
    "# 原始数据处理\n",
    "book_info , _, _ = get_book_info(book_info_path, False)\n",
    "user_info , _, _ = get_user_info(user_info_path, True)\n",
    "rating_info = get_rating_info(rating_info_path)\n",
    "\n",
    "max_user_id = len(user_info)\n",
    "\n",
    "for user_id in rating_info:\n",
    "    user_info[user_id]['preferred_books'] = []\n",
    "    for book_id in rating_info[user_id]:\n",
    "        user_info[user_id]['preferred_books'].append(book_id)\n",
    "    # user_info[user_id]['preferred_books'].sort()\n",
    "    if len(user_info[user_id]['preferred_books']) == 0:\n",
    "        user_info[user_id]['preferred_books'].append(0)\n",
    "    user_info[user_id]['preferred_books'] = user_info[user_id]['preferred_books'][:15]\n",
    "\n",
    "user_raw_info , _, _ = get_user_info(user_info_path, False)\n",
    "\n",
    "# 根据用户ID，找到已评分图书最高的前top个评分信息\n",
    "def top_user_graded_books (user, top):\n",
    "    # 获得评分过的图书ID\n",
    "    user_rating_info = {}\n",
    "\n",
    "    for key in rating_info.keys():\n",
    "        if key == user :\n",
    "            for book_id in rating_info[key] :\n",
    "                user_rating_info[book_id] = rating_info[key][book_id]\n",
    "\n",
    "    # 获得评分过的图书ID\n",
    "    book_ids = list(user_rating_info.keys())\n",
    "\n",
    "    if len(book_ids) < top: \n",
    "        top = len(book_ids)\n",
    "\n",
    "    print(\"\\n用户已评分过{}本图书 评分排行前{}的图书是: \".format(len(book_ids), top))\n",
    "\n",
    "    # 选出ID为user评分最高的前top个图书\n",
    "    ratings_tops = sorted(user_rating_info.items(), key=lambda item:item[1])[-top:]\n",
    "\n",
    "    ratings_tops = list(reversed(ratings_tops))\n",
    "\n",
    "    for k, score in ratings_tops:\n",
    "        print(\"    图书ID: {}，评分是: {}, 图书信息: {}\".format(k, score, book_info[k]))\n",
    "    graded_books = []\n",
    "    for ratings_top in ratings_tops:\n",
    "        graded_books.append(ratings_top[0])\n",
    "    return graded_books\n",
    "\n",
    "# 基于内容过滤的图书推荐\n",
    "def similarity_book_for_book_feat(user_id, book_id):\n",
    "    # 读取图书和用户的特征\n",
    "    book_feats = pickle.load(open(book_raw_feat, 'rb'))\n",
    "    similarity_books = []\n",
    "\n",
    "    cos_sims = []\n",
    "    id_feat = book_feats[str(book_id)]\n",
    "\n",
    "    # with dygraph.guard():\n",
    "    paddle.disable_static()\n",
    "    # 索引图书特征，计算和输入图书ID的特征的相似度\n",
    "    for idx, key in enumerate(book_feats.keys()):\n",
    "        book_feat = book_feats[key]\n",
    "        book_feat = paddle.to_tensor(book_feat)\n",
    "        book_id_feat = paddle.to_tensor(id_feat)\n",
    "        # 计算余弦相似度\n",
    "        sim = paddle.nn.functional.common.cosine_similarity(book_feat, book_id_feat)\n",
    "    \n",
    "        cos_sims.append(sim.numpy()[0])\n",
    "\n",
    "        # 对相似度排序\n",
    "    index = np.argsort(cos_sims)[-20:]\n",
    "    index = list(reversed(index))\n",
    "\n",
    "    for book_id in index:\n",
    "        similarity_books.append(str(book_id + 1))\n",
    "    \n",
    "    for book_id in user_info[user_id]['preferred_books']:\n",
    "        for similarity_book_id in similarity_books:\n",
    "            if str(book_id) == similarity_book_id :\n",
    "                similarity_books.remove(similarity_book_id)\n",
    "\n",
    "    # 去除列表中相同的元素\n",
    "    similarity_books = list(set(similarity_books))\n",
    "\n",
    "    while len(similarity_books) > 10 :\n",
    "        similarity_books.pop(random.randint(0,len(similarity_books)-1))\n",
    "\n",
    "    print(\"\\n 根据用户已评分图书推荐的图书有：\")\n",
    "    for book_id in similarity_books:\n",
    "        print(\"    book_id:{}\".format(book_info[book_id]))\n",
    "\n",
    "# 根据相似用户的喜好推荐图书\n",
    "def similarity_user_for_user(user_id):\n",
    "    # 读取图书和用户的特征\n",
    "    user_feats = pickle.load(open(user_raw_feat, 'rb'))\n",
    "    user_id_feat = user_feats[str(user_id)]\n",
    "    preferred_books= []\n",
    "    cos_sims = []\n",
    "    \n",
    "    # with dygraph.guard():\n",
    "    paddle.disable_static()\n",
    "    # 索引图书特征，计算和输入用户ID的特征的相似度\n",
    "    for idx, key in enumerate(user_feats.keys()):\n",
    "\n",
    "        user_feat = user_feats[key]\n",
    "        user_feat = paddle.to_tensor(user_feat)\n",
    "        user_id_feat = paddle.to_tensor(user_id_feat)\n",
    "        # 计算余弦相似度\n",
    "        sim = paddle.nn.functional.common.cosine_similarity(user_feat, user_id_feat)\n",
    "        \n",
    "        cos_sims.append(sim.numpy()[0])\n",
    "    # 对相似度排序\n",
    "    index = np.argsort(cos_sims)[-6:]\n",
    "    print(\"\\n用户ID为 {} 的用户，相似用户是：\".format(user_id))    \n",
    "    index = list(reversed(index))\n",
    "\n",
    "    for i in index: \n",
    "        if i == int(user_id)-1 :\n",
    "            continue\n",
    "        print(\"    user_id:{}\".format(user_raw_info[list(user_feats.keys())[i]]))\n",
    "        for graded_book in user_info[list(user_feats.keys())[i]]['preferred_books']:\n",
    "            preferred_books.append(graded_book)\n",
    "\n",
    "    # 去除列表中相同的元素\n",
    "    preferred_books = list(set(preferred_books))\n",
    "\n",
    "    while len(preferred_books) > 10 :\n",
    "        preferred_books.pop(random.randint(0,len(preferred_books)-1))\n",
    "\n",
    "    print(\"\\n 根据相似用户的已评分图书推荐的图书有：\")\n",
    "    for book_id in preferred_books:\n",
    "        print(\"    book_id:{}\".format(book_info[book_id]))\n",
    "\n",
    "# 相似爱好的用户\n",
    "def similarity_user_for_user_preferred_books(user_id):\n",
    "    preferred_books = []\n",
    "    sims = []\n",
    "\n",
    "    for i in user_info.keys():\n",
    "        sim = difflib.SequenceMatcher(None,user_info[i]['preferred_books'],user_info[user_id]['preferred_books'])\n",
    "        \n",
    "        sims.append(sim.ratio())\n",
    "    # 对相似度排序\n",
    "    index = np.argsort(sims)[-6:]\n",
    "\n",
    "    print(\"\\n与用户{}有相似喜好的用户有：\".format(user_id))\n",
    "\n",
    "    index = list(reversed(index))\n",
    "\n",
    "    for i in index: \n",
    "        if i == int(user_id)-1 :\n",
    "            continue\n",
    "        print(\"    user_id:{}\".format(user_raw_info[list(user_info.keys())[i]]))\n",
    "        for book_id in user_info[list(user_info.keys())[i]]['preferred_books']:\n",
    "            preferred_books.append(book_id)\n",
    "\n",
    "    # 去除列表中相同的元素\n",
    "    preferred_books = list(set(preferred_books))\n",
    "    # preferred_books.remove(0)\n",
    "\n",
    "    while len(preferred_books) > 10 :\n",
    "        preferred_books.pop(random.randint(0,len(preferred_books)-1))\n",
    "\n",
    "    for book_id in user_info[user_id]['preferred_books']:\n",
    "        for graded_book in preferred_books:\n",
    "            if str(book_id) == graded_book :\n",
    "                preferred_books.remove(book_id)\n",
    "\n",
    "    # if len(preferred_books) > 3 :\n",
    "    print(\"\\n 根据有相似喜好的用户的喜好图书推荐的图书有：\")\n",
    "    for book_id in preferred_books:\n",
    "        print(\"    book_id:{}\".format(book_info[book_id]))\n",
    "\n",
    "\n",
    "# 热门图书推荐\n",
    "def hot_book_recommendation() :\n",
    "\n",
    "    # 根据热门推荐的图书字典\n",
    "    preferred_books_list = {}\n",
    "\n",
    "    recommended_books = {}\n",
    "\n",
    "    for user_id in rating_info.keys():\n",
    "        for book_id in rating_info[user_id]:\n",
    "            if rating_info[user_id][book_id] > 3:\n",
    "                if book_id in preferred_books_list.keys():\n",
    "                    preferred_books_list[book_id] += 1\n",
    "                else:\n",
    "                    preferred_books_list[book_id] = 1\n",
    "    \n",
    "    while len(recommended_books) < 30:\n",
    "\n",
    "        max_num = 0\n",
    "\n",
    "        for book_id in preferred_books_list.keys():\n",
    "            max_num = max(max_num, preferred_books_list[book_id])\n",
    "\n",
    "        del_id = []\n",
    "\n",
    "        for book_id in preferred_books_list.keys():\n",
    "            if preferred_books_list[book_id] == max_num:\n",
    "                recommended_books[book_id] = preferred_books_list[book_id]\n",
    "                del_id.append(book_id)\n",
    "\n",
    "        for book_id in del_id:\n",
    "            preferred_books_list.pop(book_id)\n",
    "            \n",
    "    print(\"\\n 热门图书推荐：\")\n",
    "    for book_id in recommended_books.keys():\n",
    "        print(book_info[book_id],\"热度：\",recommended_books[book_id])\n",
    "\n",
    "# 通过训练模型得出的特征向量推荐图书\n",
    "def recommend_book_for_user(user_id, top_k,user_feat_dir, book_feat_dir):\n",
    "    # 读取图书和用户的特征\n",
    "    user_feats = pickle.load(open(user_feat_dir, 'rb'))\n",
    "    book_feats = pickle.load(open(book_feat_dir, 'rb'))\n",
    "    user_feat = user_feats[str(user_id)]\n",
    "\n",
    "    cos_sims = []\n",
    "    \n",
    "    # with dygraph.guard():\n",
    "    paddle.disable_static()\n",
    "    # 索引图书特征，计算和输入用户ID的特征的相似度\n",
    "    for idx, key in enumerate(book_feats.keys()):\n",
    "        book_feat = book_feats[key]\n",
    "        user_feat = paddle.to_tensor(user_feat)\n",
    "        book_feat = paddle.to_tensor(book_feat)\n",
    "        # 计算余弦相似度\n",
    "        sim = paddle.nn.functional.common.cosine_similarity(user_feat, book_feat)\n",
    "        \n",
    "        cos_sims.append(sim.numpy()[0])\n",
    "    # 对相似度排序\n",
    "    index = np.argsort(cos_sims)[-top_k:]\n",
    "    \n",
    "    index = list(reversed(index))\n",
    "\n",
    "    print(\"\\n 通过训练模型得出的特征向量推荐图书：\")\n",
    "    for i in index :\n",
    "        print(book_info[list(book_feats.keys())[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户ID为 39138 的用户，用户信息为:{'user_id': 39138, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}  \n",
      "\n",
      "用户已评分过24本图书 评分排行前10的图书是: \n",
      "    图书ID: 62043，评分是: 5.0, 图书信息: {'book_id': 62043, '图书标题': '蒙塔莱诗选', '类别': ['文学', '诗歌', '诗', '意大利']}\n",
      "    图书ID: 31636，评分是: 5.0, 图书信息: {'book_id': 31636, '图书标题': '后垮掉派诗选', '类别': ['外国文学', '美国', '诗歌', '诗']}\n",
      "    图书ID: 50981，评分是: 5.0, 图书信息: {'book_id': 50981, '图书标题': '鲁达基、海亚姆、萨迪、哈菲兹作品选', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    图书ID: 60367，评分是: 5.0, 图书信息: {'book_id': 60367, '图书标题': '寂寞的人坐着看花', '类别': ['文学', '诗歌', '台湾', '诗']}\n",
      "    图书ID: 34680，评分是: 5.0, 图书信息: {'book_id': 34680, '图书标题': '驶向拜占庭', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    图书ID: 53714，评分是: 5.0, 图书信息: {'book_id': 53714, '图书标题': '从彼得堡到斯德哥尔摩', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    图书ID: 62742，评分是: 5.0, 图书信息: {'book_id': 62742, '图书标题': '郭小川诗选', '类别': ['文学', '诗歌', '诗词', '诗']}\n",
      "    图书ID: 2819，评分是: 5.0, 图书信息: {'book_id': 2819, '图书标题': '我们无法猜出的谜--狄金森选集', '类别': ['文学', '美国', '诗歌', '诗']}\n",
      "    图书ID: 3573，评分是: 5.0, 图书信息: {'book_id': 3573, '图书标题': '苇间风', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    图书ID: 2561，评分是: 5.0, 图书信息: {'book_id': 2561, '图书标题': '食指的诗', '类别': ['中国', '文学', '诗歌', '诗']}\n"
     ]
    }
   ],
   "source": [
    "# 从相似度排序中获取top_k个结果推荐给用户\n",
    "top_k= 20\n",
    "top = 10\n",
    "\n",
    "# 需要推荐图书用户ID\n",
    "user_id = random.randint(1,max_user_id)\n",
    "    \n",
    "print(\"用户ID为 {} 的用户，用户信息为:{}  \".format(user_id, user_raw_info[str(user_id)]))\n",
    "\n",
    "# 获取用户喜好\n",
    "graded_books = top_user_graded_books(str(user_id), top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于内容过滤的图书推荐——根据用户喜好图书推荐\n",
    "基于内容过滤的图书推荐——根据用户已经评了高分的图书来推荐图书，通过计算用户已评分图书特征和其他所有保存在文件中的图书特征之间的相似度，对所有计算得出的相似度进行排序，把相识度较高的几本图书作为推荐图书。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3ef9f92bb3ab42f59fc5de59b933beee943ad5fa3023417dbccb15bf4f62559f\"></center>\n",
    "<center><br>图6：根据用户喜好图书推荐的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 根据用户已评分图书推荐的图书有：\n",
      "    book_id:{'book_id': 55773, '图书标题': '高迪', '类别': ['艺术', '设计', '建筑', '西班牙']}\n",
      "    book_id:{'book_id': 6644, '图书标题': '诗经全译', '类别': ['文学', '诗歌', '国学', '诗']}\n",
      "    book_id:{'book_id': 1770, '图书标题': '波德莱尔诗全集', '类别': ['文学', '诗歌', '法国', '诗']}\n",
      "    book_id:{'book_id': 5493, '图书标题': '双子座建筑艺术丛书（全4种）', '类别': ['艺术', '建筑', '画册', '西班牙']}\n",
      "    book_id:{'book_id': 57378, '图书标题': '一生要读的60首诗歌', '类别': ['文学', '诗歌', '2006', '随便看看']}\n",
      "    book_id:{'book_id': 62043, '图书标题': '蒙塔莱诗选', '类别': ['文学', '诗歌', '诗', '意大利']}\n",
      "    book_id:{'book_id': 2112, '图书标题': '高迪的房子', '类别': ['艺术', '设计', '建筑', '西班牙']}\n",
      "    book_id:{'book_id': 43750, '图书标题': '另一種聲音', '类别': ['文学', '诗歌', '诗', '现当代']}\n",
      "    book_id:{'book_id': 48029, '图书标题': '达利谈话录', '类别': ['艺术', '传记', '访谈', '西班牙']}\n",
      "    book_id:{'book_id': 69734, '图书标题': '新诗经', '类别': ['诗歌', '诗', '非动态出', '中国新诗']}\n"
     ]
    }
   ],
   "source": [
    "# 根据用户喜好图书推荐\n",
    "similarity_book_for_book_feat(str(user_id), graded_books[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于协同过滤的图书推荐——根据相似用户的喜好图书进行推荐图书\n",
    "基于协同过滤的图书推荐——根据相似用户的喜好图书进行推荐图书，通过计算需要图书推荐的用户特征和其他文件里保存的所有用户特征之间的相似度，对通过余弦相似度计算方式计算得出相似度进行排序，取相似度较大的几个用户所喜爱的图书作为推荐图书。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bed149833d9341bd9ac4d1e3ec70c3f271afa1eb423b4c99814a9187bd137c30\"></center>\n",
    "<center><br>图7：根据相似用户的喜好图书进行推荐图书的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户ID为 39138 的用户，相似用户是：\n",
      "    user_id:{'user_id': 50107, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}\n",
      "    user_id:{'user_id': 25506, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}\n",
      "    user_id:{'user_id': 4826, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}\n",
      "    user_id:{'user_id': 26025, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}\n",
      "    user_id:{'user_id': 34140, '性别': '男', '年龄': '66', '职业': '电子报/电子杂志编辑'}\n",
      "\n",
      " 根据相似用户的已评分图书推荐的图书有：\n",
      "    book_id:{'book_id': 24089, '图书标题': '让思想冲破牢笼', '类别': ['管理', '思维', '管理学', '创新']}\n",
      "    book_id:{'book_id': 45209, '图书标题': '这，就是华尔街', '类别': ['小说', '传记', '金融', '投资']}\n",
      "    book_id:{'book_id': 37895, '图书标题': '柔韧', '类别': ['管理', '商业', '思维', '经管']}\n",
      "    book_id:{'book_id': 10733, '图书标题': '黎塞留传(全两册)', '类别': ['历史', '传记', '政治', '法国']}\n",
      "    book_id:{'book_id': 17793, '图书标题': '日本包装百例', '类别': ['日本', '艺术', '设计', 'design']}\n",
      "    book_id:{'book_id': 5051, '图书标题': '世界设计大师图典', '类别': ['艺术', '设计', '画册', 'design']}\n",
      "    book_id:{'book_id': 56393, '图书标题': '我的废纸', '类别': ['艺术', '设计', '平面设计', '設計']}\n",
      "    book_id:{'book_id': 38785, '图书标题': 'Grid Index', '类别': ['艺术', '设计', '德国', 'design']}\n",
      "    book_id:{'book_id': 13387, '图书标题': '创造金钱  长期资本管理公司的传奇', '类别': ['传记', '经济', '金融', '投资']}\n",
      "    book_id:{'book_id': 50586, '图书标题': '首饰设计', '类别': ['艺术', '设计', 'design', '首饰设计']}\n"
     ]
    }
   ],
   "source": [
    "# 根据相似用户的喜好图书推荐\n",
    "similarity_user_for_user(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于协同过滤的图书推荐——根据有相似喜好的用户的喜好图书进行推荐图书\n",
    "基于协同过滤的图书推荐——根据有相似喜好的用户的喜好图书进行推荐图书，通过计算用户的爱好特征和其他用户的喜好特征之间的相似度，对通过余弦相似度计算方式计算得出相似度进行排序，取喜好图书相似度较大的几个用户所喜爱的其他图书作为推荐图书。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/afef6c4084db4fadab665c9b6bcf219d6153ccd4159b4036b66f21c5c2a1efca\"></center>\n",
    "<center><br>图8：根据有相似喜好的用户的喜好图书进行推荐图书的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "与用户39138有相似喜好的用户有：\n",
      "    user_id:{'user_id': 43439, '性别': '男', '年龄': '34', '职业': '宠物美容师'}\n",
      "    user_id:{'user_id': 17908, '性别': '男', '年龄': '61', '职业': '护士'}\n",
      "    user_id:{'user_id': 15399, '性别': '男', '年龄': '67', '职业': '制程工程师'}\n",
      "    user_id:{'user_id': 29438, '性别': '男', '年龄': '65', '职业': '陶瓷技师'}\n",
      "    user_id:{'user_id': 38219, '性别': '男', '年龄': '39', '职业': '木雕工'}\n",
      "\n",
      " 根据有相似喜好的用户的喜好图书推荐的图书有：\n",
      "    book_id:{'book_id': 28717, '图书标题': '飞鸟集(中英对照)/大家译丛', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    book_id:{'book_id': 62742, '图书标题': '郭小川诗选', '类别': ['文学', '诗歌', '诗词', '诗']}\n",
      "    book_id:{'book_id': 50981, '图书标题': '鲁达基、海亚姆、萨迪、哈菲兹作品选', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    book_id:{'book_id': 24520, '图书标题': '雷聲與蟬鳴', '类别': ['文学', '诗歌', '香港', '诗']}\n",
      "    book_id:{'book_id': 31109, '图书标题': '游思集', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    book_id:{'book_id': 60617, '图书标题': '秀发遭劫记', '类别': ['文学', '外国文学', '诗歌', '诗']}\n",
      "    book_id:{'book_id': 29800, '图书标题': '新诗一百句', '类别': ['文学', '中国文学', '诗歌', '诗']}\n",
      "    book_id:{'book_id': 19469, '图书标题': 'The Poetry of Pablo Neruda', '类别': ['文学', '诗歌', '诗', '拉美文学']}\n",
      "    book_id:{'book_id': 41452, '图书标题': '界限诗丛( 12诗人诗集)', '类别': ['文学', '诗歌', '诗', '文艺']}\n"
     ]
    }
   ],
   "source": [
    "# 根据相似爱好用户的喜好图书推荐\n",
    "similarity_user_for_user_preferred_books(str(user_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 根据训练模型的结果推荐图书\n",
    "根据训练模型的结果推荐图书，通过计算经过训练的用户特征和其他所有保存在文件中训练过的图书特征之间的相似度，对所有计算得出的相似度进行排序，把相识度较高的几本图书作为推荐图书。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9f85a7ddf0ee47209fdf865400d4d7451711206a57bf4066bdff208aab588da9\"></center>\n",
    "<center><br>图9：根据训练模型的结果推荐图书的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 通过训练模型得出的特征向量推荐图书：\n",
      "{'book_id': 19379, '图书标题': '心藏大恶', '类别': ['中国', '诗歌', '诗', '中国当代文学']}\n",
      "{'book_id': 14707, '图书标题': '被遗忘的经典诗歌（上下）', '类别': ['文学', '经典', '诗歌', '诗']}\n",
      "{'book_id': 51252, '图书标题': '自由诗篇', '类别': ['文学', '经典', '诗歌', '诗']}\n",
      "{'book_id': 8692, '图书标题': '人一生要读的60首诗歌', '类别': ['文学', '经典', '诗歌', '诗']}\n",
      "{'book_id': 53274, '图书标题': 'The Best Poems of the English Language', '类别': ['文学', '诗歌', '诗', '英语']}\n",
      "{'book_id': 31671, '图书标题': '被阉割的文明', '类别': ['历史', '中国', '社会', '社科']}\n",
      "{'book_id': 38187, '图书标题': '中国30年', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 11176, '图书标题': '文化大革命简史', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 23125, '图书标题': '明朝十讲', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 6041, '图书标题': '观察中国', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 31460, '图书标题': '中国近代史', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 52593, '图书标题': '《仪礼》与《礼记》之社会学的研究', '类别': ['历史', '社会学', '人类学', '史学']}\n",
      "{'book_id': 3182, '图书标题': '25年', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 14639, '图书标题': '中国历史通论', '类别': ['历史', '中国', '社会', '中国历史']}\n",
      "{'book_id': 8886, '图书标题': '醒世恒言', '类别': ['小说', '文学', '古典文学', '社会']}\n",
      "{'book_id': 31347, '图书标题': '儒林外史', '类别': ['小说', '文学', '古典文学', '社会']}\n",
      "{'book_id': 14529, '图书标题': '约翰·阿什贝利诗选', '类别': ['文学', '外国文学', '美国', '诗歌']}\n",
      "{'book_id': 63854, '图书标题': '美国现代诗选', '类别': ['文学', '外国文学', '美国', '诗歌']}\n",
      "{'book_id': 37801, '图书标题': '灵魂深处有个鬼', '类别': ['中国', '诗歌', '诗', '诗选']}\n",
      "{'book_id': 33065, '图书标题': '暴风雨夜，暴风雨夜', '类别': ['文学', '外国文学', '美国', '诗歌']}\n"
     ]
    }
   ],
   "source": [
    "# 通过训练模型得出的特征向量推荐图书\n",
    "recommend_book_for_user(user_id, top_k, user_train_feat, book_train_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 热门图书推荐\n",
    "热门图书推荐会把所有图书中最受欢迎的图书推荐给用户，通过统计所有用户喜爱的图书找出最受欢迎的图书作为推荐结果推荐给用户。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/992f4311c4e044358c6a9205ad99f5d697d46e005f0145a2a4c2fb1e4761b21f\"></center>\n",
    "<center><br>图10：热门图书推荐的流程图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 热门图书推荐：\n",
      "{'book_id': 61208, '图书标题': 'たかみち画集', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 317\n",
      "{'book_id': 42375, '图书标题': '带你去巴黎', '类别': ['外国文学', '随笔', '生活', '英国']} 热度： 316\n",
      "{'book_id': 61206, '图书标题': 'Food Girls', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 315\n",
      "{'book_id': 27563, '图书标题': 'formcode range murata 3rd drawing works limited edition 24+20', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 314\n",
      "{'book_id': 25915, '图书标题': 'INNOCENCE―美樹本晴彦画集', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 313\n",
      "{'book_id': 66548, '图书标题': '宫崎骏1968至2008年手稿', '类别': ['日本', '绘本', '画册', '画集']} 热度： 308\n",
      "{'book_id': 31205, '图书标题': 'Yours', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 308\n",
      "{'book_id': 14853, '图书标题': 'INOUE TAKEHIKO ILLUSTRATIONS', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 306\n",
      "{'book_id': 35434, '图书标题': '山田章博の世界―ミスティックアークアートワークス', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 306\n",
      "{'book_id': 34029, '图书标题': '陰陽師 天野喜孝コンセプトデザイン集', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 304\n",
      "{'book_id': 64457, '图书标题': '殉教者のためのディヴェルティメント', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 303\n",
      "{'book_id': 62244, '图书标题': 'NieA_7 Scrap', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 298\n",
      "{'book_id': 58945, '图书标题': 'Sandman:The Dream Hunters', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 296\n",
      "{'book_id': 45117, '图书标题': 'pixiv girls collection 2010～ピクシブガールズコレクション2010', '类别': ['日本', '绘本', '画册', '画集']} 热度： 296\n",
      "{'book_id': 26134, '图书标题': '梦想之旅', '类别': ['随笔', '英国', '旅行', '游记']} 热度： 295\n",
      "{'book_id': 21495, '图书标题': 'lain‐安倍吉俊画集', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 295\n",
      "{'book_id': 29043, '图书标题': '以温柔优雅的态度生活', '类别': ['随笔', '散文', '生活', '女性']} 热度： 292\n",
      "{'book_id': 43607, '图书标题': '萌える日本刀大全', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 288\n",
      "{'book_id': 10424, '图书标题': '玫瑰物语', '类别': ['随笔', '散文', '生活', '女性']} 热度： 287\n",
      "{'book_id': 22634, '图书标题': '电影理论史评', '类别': ['美国', '电影', '电影理论', '考研']} 热度： 284\n",
      "{'book_id': 33646, '图书标题': '新海誠美術作品集 空の記憶~The sky of the longing for memories~', '类别': ['日本', '绘本', '画册', '画集']} 热度： 284\n",
      "{'book_id': 20921, '图书标题': '上海熟女', '类别': ['随笔', '散文', '生活', '女性']} 热度： 282\n",
      "{'book_id': 21733, '图书标题': 'Cannabis Works', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 281\n",
      "{'book_id': 65442, '图书标题': 'LO画集 TAKAMICHI LOVE WORKS', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 280\n",
      "{'book_id': 23487, '图书标题': '画集 陰陽師', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 278\n",
      "{'book_id': 70002, '图书标题': '図説女子高制服百科', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 278\n",
      "{'book_id': 30324, '图书标题': '蟲襖', '类别': ['日本', '漫画', '绘本', '画集']} 热度： 277\n",
      "{'book_id': 9491, '图书标题': '纯棉女友', '类别': ['随笔', '散文', '生活', '女性']} 热度： 273\n",
      "{'book_id': 5574, '图书标题': '寻找回来的世界', '类别': ['随笔', '英国', '旅行', '游记']} 热度： 268\n",
      "{'book_id': 3884, '图书标题': '剑桥流水', '类别': ['随笔', '英国', '旅行', '游记']} 热度： 268\n"
     ]
    }
   ],
   "source": [
    "# 热门图书推荐\n",
    "hot_book_recommendation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
