{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入系统库\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 导入Paddle库\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle import inference\n",
    "\n",
    "# 导入PaddleNLP相关的库\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 忽略所有警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 有监督语义索引\n",
    "#### 数据准备\n",
    "\n",
    "使用文献的的query, title, keywords，构造带正标签的数据集，不包含负标签样本\n",
    "\n",
    "```\n",
    "宁夏社区图书馆服务体系布局现状分析\t       宁夏社区图书馆服务体系布局现状分析社区图书馆,社区图书馆服务,社区图书馆服务体系\n",
    "人口老龄化对京津冀经济\t                 京津冀人口老龄化对区域经济增长的影响京津冀,人口老龄化,区域经济增长,固定效应模型\n",
    "英语广告中的模糊语\t                  模糊语在英语广告中的应用及其功能模糊语,英语广告,表现形式,语用功能\n",
    "甘氨酸二肽的合成\t                      甘氨酸二肽合成中缩合剂的选择甘氨酸,缩合剂,二肽\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_text_pair(data_path):\n",
    "    \"\"\"Reads data.\"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.rstrip().split(\"\\t\")\n",
    "            if len(data) != 2:\n",
    "                continue\n",
    "            # 可以看到有监督数据使用query title pair的\n",
    "            # 所以text_a和text_b不一样\n",
    "            yield {'text_a': data[0], 'text_b': data[1]}\n",
    "\n",
    "train_set_file='./data/train.csv'\n",
    "train_ds = load_dataset(\n",
    "        read_text_pair, data_path=train_set_file, lazy=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 在训练神经网络之前，我们需要构建小批量的数据，所以需要借助Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        # 分布式批采样器加载数据的一个子集。\n",
    "        # 每个进程可以传递给DataLoader一个DistributedBatchSampler的实例，每个进程加载原始数据的一个子集。\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        # 批采样器的基础实现，\n",
    "        # 用于 paddle.io.DataLoader 中迭代式获取mini-batch的样本下标数组，数组长度与 batch_size 一致。\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    # 组装mini-batch\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=512, do_evalute=False):\n",
    "    result = []\n",
    "\n",
    "    for key, text in example.items():\n",
    "        if 'label' in key:\n",
    "            # do_evaluate\n",
    "            result += [example['label']]\n",
    "        else:\n",
    "            # do_train\n",
    "            encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "            input_ids = encoded_inputs[\"input_ids\"]\n",
    "            token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "            result += [input_ids, token_type_ids]\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 模型构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.base_model import SemanticIndexBase\n",
    "\n",
    "class SemanticIndexBatchNeg(SemanticIndexBase):\n",
    "    def __init__(self,\n",
    "                 pretrained_model,\n",
    "                 dropout=None,\n",
    "                 margin=0.3,\n",
    "                 scale=30,\n",
    "                 output_emb_size=None):\n",
    "        super().__init__(pretrained_model, dropout, output_emb_size)\n",
    "\n",
    "        self.margin = margin\n",
    "        # Used scaling cosine similarity to ease converge\n",
    "        self.sacle = scale\n",
    "\n",
    "    def forward(self,\n",
    "                query_input_ids,\n",
    "                title_input_ids,\n",
    "                query_token_type_ids=None,\n",
    "                query_position_ids=None,\n",
    "                query_attention_mask=None,\n",
    "                title_token_type_ids=None,\n",
    "                title_position_ids=None,\n",
    "                title_attention_mask=None):\n",
    "\n",
    "        query_cls_embedding = self.get_pooled_embedding(\n",
    "            query_input_ids, query_token_type_ids, query_position_ids,\n",
    "            query_attention_mask)\n",
    "\n",
    "        title_cls_embedding = self.get_pooled_embedding(\n",
    "            title_input_ids, title_token_type_ids, title_position_ids,\n",
    "            title_attention_mask)\n",
    "\n",
    "        cosine_sim = paddle.matmul(\n",
    "            query_cls_embedding, title_cls_embedding, transpose_y=True)\n",
    "\n",
    "        # substract margin from all positive samples cosine_sim()\n",
    "        margin_diag = paddle.full(\n",
    "            shape=[query_cls_embedding.shape[0]],\n",
    "            fill_value=self.margin,\n",
    "            dtype=paddle.get_default_dtype())\n",
    "\n",
    "        cosine_sim = cosine_sim - paddle.diag(margin_diag)\n",
    "\n",
    "        # scale cosine to ease training converge\n",
    "        cosine_sim *= self.sacle\n",
    "\n",
    "        labels = paddle.arange(0, query_cls_embedding.shape[0], dtype='int64')\n",
    "        labels = paddle.reshape(labels, shape=[-1, 1])\n",
    "\n",
    "        loss = F.cross_entropy(input=cosine_sim, label=labels)\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 训练配置\n",
    "\n",
    "定义模型训练的超参，优化器等等。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 关键参数\n",
    "scale=20 # 推荐值: 10 ~ 30\n",
    "margin=0.1 # 推荐值: 0.0 ~ 0.2\n",
    "# 最大序列长度\n",
    "max_seq_length=64\n",
    "epochs=1\n",
    "learning_rate=5E-5\n",
    "warmup_proportion=0.0\n",
    "weight_decay=0.0\n",
    "save_steps=10\n",
    "batch_size=64\n",
    "output_emb_size=256"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = ppnlp.transformers.ErnieModel.from_pretrained(\n",
    "        'ernie-3.0-medium-zh')\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained('ernie-3.0-medium-zh')\n",
    "trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # query_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  # query_segment\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # title_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  # tilte_segment\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "# Inbatch-Negatives\n",
    "model = SemanticIndexBatchNeg(\n",
    "        pretrained_model,\n",
    "        margin=margin,\n",
    "        scale=scale,\n",
    "        output_emb_size=output_emb_size)\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_proportion)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 模型训练\n",
    "\n",
    "模型训练过程如下：\n",
    "\n",
    "1.从dataloader中取出小批量数据\n",
    "\n",
    "2.输入到模型中做前向\n",
    "\n",
    "3.求损失函数\n",
    "\n",
    "3.反向传播更新梯度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir='checkpoint'\n",
    "\n",
    "def do_train(model,train_data_loader):\n",
    "\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch\n",
    "\n",
    "            loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids,\n",
    "                query_token_type_ids=query_token_type_ids,\n",
    "                title_token_type_ids=title_token_type_ids)\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 5 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            if global_step % save_steps == 0:\n",
    "                save_path = os.path.join(save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                save_param_path = os.path.join(save_path, 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                tokenizer.save_pretrained(save_path)\n",
    "\n",
    "do_train(model,train_data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 模型预测\n",
    "\n",
    "模型预测部分加载训练好的模型，然后输入两条示例数据进行预测抽取向量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.data import convert_example_test\n",
    "max_seq_length=64\n",
    "output_emb_size=256\n",
    "batch_size=1\n",
    "\n",
    "pretrained_model = ppnlp.transformers.ErnieModel.from_pretrained(\n",
    "        'ernie-1.0')\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "model = SemanticIndexBatchNeg(\n",
    "        pretrained_model,\n",
    "        margin=margin,\n",
    "        scale=scale,\n",
    "        output_emb_size=output_emb_size)\n",
    "params_path='./pretrained/model_40/model_state.pdparams'\n",
    "test_data = [\"国有企业引入非国有资本对创新绩效的影响——基于制造业国有上市公司的经验证据\"]\n",
    "# 加载模型\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "test_func = partial(\n",
    "        convert_example_test,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "\n",
    "test_batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # text_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  # text_segment\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "# conver_example function's input must be dict\n",
    "corpus_ds = MapDataset(test_data)\n",
    "\n",
    "corpus_data_loader = create_dataloader(\n",
    "        corpus_ds,\n",
    "        mode='predict',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=test_batchify_fn,\n",
    "        trans_fn=test_func)\n",
    "\n",
    "all_embeddings = []\n",
    "model.eval()\n",
    "with paddle.no_grad():\n",
    "    for batch_data in corpus_data_loader:\n",
    "        input_ids, token_type_ids = batch_data\n",
    "        input_ids = paddle.to_tensor(input_ids, dtype='int64')\n",
    "        token_type_ids = paddle.to_tensor(token_type_ids, dtype='int64')\n",
    "        text_embeddings = model.get_pooled_embedding(input_ids, token_type_ids)\n",
    "        all_embeddings.append(text_embeddings)\n",
    "\n",
    "text_embedding=all_embeddings[0]\n",
    "print(text_embedding.shape)\n",
    "print(text_embedding.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型部署\n",
    "\n",
    "模型部署首先需要把模型转换成静态图模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_path='./output/recall'\n",
    "model.eval()\n",
    "# Convert to static graph with specific input description\n",
    "model = paddle.jit.to_static(\n",
    "        model,\n",
    "        input_spec=[\n",
    "            paddle.static.InputSpec(\n",
    "                shape=[None, None], dtype=\"int64\"),  # input_ids\n",
    "            paddle.static.InputSpec(\n",
    "                shape=[None, None], dtype=\"int64\")  # segment_ids\n",
    "        ])\n",
    "# Save in static graph model.\n",
    "save_path = os.path.join(output_path, \"inference\")\n",
    "print(save_path)\n",
    "paddle.jit.save(model, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.data import convert_example_recall_infer\n",
    "from scipy.special import softmax\n",
    "from scipy import spatial\n",
    "\n",
    "class RecallPredictor(object):\n",
    "    def __init__(self,\n",
    "                 model_dir,\n",
    "                 device=\"gpu\",\n",
    "                 max_seq_length=128,\n",
    "                 batch_size=32,\n",
    "                 use_tensorrt=False,\n",
    "                 precision=\"fp32\",\n",
    "                 cpu_threads=10,\n",
    "                 enable_mkldnn=False):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        model_file = model_dir + \"/output/recall/inference.get_pooled_embedding.pdmodel\"\n",
    "        params_file = model_dir + \"/output/recall/inference.get_pooled_embedding.pdiparams\"\n",
    "        if not os.path.exists(model_file):\n",
    "            raise ValueError(\"not find model file path {}\".format(model_file))\n",
    "        if not os.path.exists(params_file):\n",
    "            raise ValueError(\"not find params file path {}\".format(params_file))\n",
    "        config = paddle.inference.Config(model_file, params_file)\n",
    "\n",
    "        if device == \"gpu\":\n",
    "            # set GPU configs accordingly\n",
    "            # such as intialize the gpu memory, enable tensorrt\n",
    "            config.enable_use_gpu(100, 0)\n",
    "            precision_map = {\n",
    "                \"fp16\": inference.PrecisionType.Half,\n",
    "                \"fp32\": inference.PrecisionType.Float32,\n",
    "                \"int8\": inference.PrecisionType.Int8\n",
    "            }\n",
    "            precision_mode = precision_map[precision]\n",
    "\n",
    "            if use_tensorrt:\n",
    "                config.enable_tensorrt_engine(\n",
    "                    max_batch_size=batch_size,\n",
    "                    min_subgraph_size=30,\n",
    "                    precision_mode=precision_mode)\n",
    "        elif device == \"cpu\":\n",
    "            # set CPU configs accordingly,\n",
    "            # such as enable_mkldnn, set_cpu_math_library_num_threads\n",
    "            config.disable_gpu()\n",
    "            if enable_mkldnn:\n",
    "                # cache 10 different shapes for mkldnn to avoid memory leak\n",
    "                config.set_mkldnn_cache_capacity(10)\n",
    "                config.enable_mkldnn()\n",
    "            config.set_cpu_math_library_num_threads(cpu_threads)\n",
    "        elif device == \"xpu\":\n",
    "            # set XPU configs accordingly\n",
    "            config.enable_xpu(100)\n",
    "\n",
    "        config.switch_use_feed_fetch_ops(False)\n",
    "        self.predictor = paddle.inference.create_predictor(config)\n",
    "        self.input_handles = [\n",
    "            self.predictor.get_input_handle(name)\n",
    "            for name in self.predictor.get_input_names()\n",
    "        ]\n",
    "        self.output_handle = self.predictor.get_output_handle(\n",
    "            self.predictor.get_output_names()[0])\n",
    "\n",
    "\n",
    "\n",
    "    def extract_embedding(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        Predicts the data labels.\n",
    "        Args:\n",
    "            data (obj:`List(str)`): The batch data whose each element is a raw text.\n",
    "            tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer`\n",
    "                which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        Returns:\n",
    "            results(obj:`dict`): All the feature vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for text in data:\n",
    "            input_ids, segment_ids = convert_example_recall_infer(text, tokenizer)\n",
    "            examples.append((input_ids, segment_ids))\n",
    "\n",
    "        batchify_fn = lambda samples, fn=Tuple(\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # input\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # segment\n",
    "        ): fn(samples)\n",
    "\n",
    "        input_ids, segment_ids = batchify_fn(examples)\n",
    "        self.input_handles[0].copy_from_cpu(input_ids)\n",
    "        self.input_handles[1].copy_from_cpu(segment_ids)\n",
    "        self.predictor.run()\n",
    "        logits = self.output_handle.copy_to_cpu()\n",
    "        return logits\n",
    "\n",
    "    def predict(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        Predicts the data labels.\n",
    "        Args:\n",
    "            data (obj:`List(str)`): The batch data whose each element is a raw text.\n",
    "            tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer`\n",
    "                which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        Returns:\n",
    "            results(obj:`dict`): All the predictions probs.\n",
    "        \"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for idx, text in enumerate(data):\n",
    "            input_ids, segment_ids = convert_example_recall_infer({idx: text[0]}, tokenizer)\n",
    "            title_ids, title_segment_ids = convert_example_recall_infer({\n",
    "                idx: text[1]\n",
    "            }, tokenizer)\n",
    "            examples.append(\n",
    "                (input_ids, segment_ids, title_ids, title_segment_ids))\n",
    "\n",
    "        batchify_fn = lambda samples, fn=Tuple(\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # input\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  # segment\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int64'),  # segment\n",
    "            Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int64'),  # segment\n",
    "        ): fn(samples)\n",
    "\n",
    "\n",
    "        query_ids, query_segment_ids, title_ids, title_segment_ids = batchify_fn(\n",
    "            examples)\n",
    "        self.input_handles[0].copy_from_cpu(query_ids)\n",
    "        self.input_handles[1].copy_from_cpu(query_segment_ids)\n",
    "        self.predictor.run()\n",
    "        query_logits = self.output_handle.copy_to_cpu()\n",
    "\n",
    "        self.input_handles[0].copy_from_cpu(title_ids)\n",
    "        self.input_handles[1].copy_from_cpu(title_segment_ids)\n",
    "        self.predictor.run()\n",
    "        title_logits = self.output_handle.copy_to_cpu()\n",
    "\n",
    "        result = [\n",
    "            float(1 - spatial.distance.cosine(arr1, arr2))\n",
    "            for arr1, arr2 in zip(query_logits, title_logits)\n",
    "        ]\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_dir = './output/recall'\n",
    "# device='gpu'\n",
    "device='cpu'\n",
    "max_seq_length=64\n",
    "use_tensorrt = False\n",
    "batch_size =32\n",
    "precision = 'fp32'\n",
    "cpu_threads = 1\n",
    "enable_mkldnn =False\n",
    "predictor = RecallPredictor(model_dir, device, max_seq_length,\n",
    "                          batch_size, use_tensorrt, precision,\n",
    "                          cpu_threads, enable_mkldnn)\n",
    "\n",
    "\n",
    "id2corpus = {0: '国有企业引入非国有资本对创新绩效的影响——基于制造业国有上市公司的经验证据'}\n",
    "corpus_list = [{idx: text} for idx, text in id2corpus.items()]\n",
    "res = predictor.extract_embedding(corpus_list, tokenizer)\n",
    "print('抽取向量')\n",
    "print(res.shape)\n",
    "print(res)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_list = [['中西方语言与文化的差异', '中西方文化差异以及语言体现中西方文化,差异,语言体现'],\n",
    "                   ['中西方语言与文化的差异', '飞桨致力于让深度学习技术的创新与应用更简单']]\n",
    "res = predictor.predict(corpus_list, tokenizer)\n",
    "print('计算相似度')\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导出静态图接下来就是部署了，目前部署支持C++和Pipeline两种方式，由于aistudio不支持部署环境，需要部署的话可以参考链接:[https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/neural_search/recall/in_batch_negative/deploy](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/neural_search/recall/in_batch_negative/deploy)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
