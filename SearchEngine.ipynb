{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************开始！***************************************************\n",
      "开始抓取第 1 个链接： https://www.aminer.cn/topic\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# from collections import deque\n",
    "# import urllib\n",
    "# from urllib import request\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# import lxml\n",
    "# # import sqlite3\n",
    "# import jieba\n",
    "#\n",
    "# safelock=input('你确定要重新构建约5000篇文档的词库吗？(y/n)')\n",
    "# if safelock!='y':\n",
    "#     sys.exit('终止。')\n",
    "#\n",
    "# url='https://www.aminer.cn/topic'#入口\n",
    "#\n",
    "# queue=deque()#待爬取链接的集合，使用广度优先搜索\n",
    "# visited=set()#已访问的链接集合\n",
    "# queue.append(url)\n",
    "#\n",
    "# # conn=sqlite3.connect('viewsdu.db')\n",
    "# # c=conn.cursor()\n",
    "# #在create table之前先drop table是因为我之前测试的时候已经建过table了，所以再次运行代码的时候得把旧的table删了重新建\n",
    "# # c.execute('drop table doc')\n",
    "# # c.execute('create table doc (id int primary key,link text)')\n",
    "# # c.execute('drop table word')\n",
    "# # c.execute('create table word (term varchar(25) primary key,list text)')\n",
    "# # conn.commit()\n",
    "# # conn.close()\n",
    "#\n",
    "# print('***************开始！***************************************************')\n",
    "# cnt=0\n",
    "#\n",
    "# while queue:\n",
    "#     url=queue.popleft()\n",
    "#     visited.add(url)\n",
    "#     cnt+=1\n",
    "#     print('开始抓取第',cnt,'个链接：',url)\n",
    "#\n",
    "#     #爬取网页内容\n",
    "#     try:\n",
    "#         response=request.urlopen(url)\n",
    "#         content=response.read().decode('gb18030')\n",
    "#     except:\n",
    "#         continue\n",
    "#\n",
    "#     #寻找下一个可爬的链接，因为搜索范围是网站内，所以对链接有格式要求，这个格式要求根据具体情况而定\n",
    "#     m=re.findall(r'<a href=\\\"([0-9a-zA-Z\\_\\/\\.\\%\\?\\=\\-\\&]+)\\\" target=\\\"_blank\\\">',content,re.I)\n",
    "#     for x in m:\n",
    "#         if re.match(r'http.+',x):\n",
    "#             if not re.match(r'http\\:\\/\\/www\\.view\\.sdu\\.edu\\.cn\\/.+',x):\n",
    "#                 continue\n",
    "#         elif re.match(r'\\/new\\/.+',x):\n",
    "#             x='http://www.view.sdu.edu.cn'+x\n",
    "#         else:\n",
    "#             x='http://www.view.sdu.edu.cn/new/'+x\n",
    "#         if (x not in visited) and (x not in queue):\n",
    "#             queue.append(x)\n",
    "#\n",
    "#     #解析网页内容,可能有几种情况,这个也是根据这个网站网页的具体情况写的\n",
    "#     soup=BeautifulSoup(content,'lxml')\n",
    "#     title=soup.title\n",
    "#     article=soup.find('div',class_='text_s',id='content')\n",
    "#     author=soup.find('div',class_='text_c')\n",
    "#\n",
    "#     if title==None and article==None and author==None:\n",
    "#         print('无内容的页面。')\n",
    "#         continue\n",
    "#\n",
    "#     elif article==None and author==None:\n",
    "#         print('只有标题。')\n",
    "#         title=title.text\n",
    "#         title=''.join(title.split())\n",
    "#         article=''\n",
    "#         author=''\n",
    "#\n",
    "#     elif article==None:\n",
    "#         print('有标题有作者，缺失内容')#视频新闻\n",
    "#         title=soup.h1.text\n",
    "#         title=''.join(title.split())\n",
    "#         article=''\n",
    "#         author=author.get_text(\"\",strip=True)\n",
    "#         author=''.join(author.split())\n",
    "#\n",
    "#     elif author==None:\n",
    "#         print('有标题有内容，缺失作者')\n",
    "#         title=soup.h1.text\n",
    "#         title=''.join(title.split())\n",
    "#         article=article.get_text(\"\",strip=True)\n",
    "#         article=''.join(article.split())\n",
    "#         author=''\n",
    "#\n",
    "#     else:\n",
    "#         title=soup.h1.text\n",
    "#         title=''.join(title.split())\n",
    "#         article=article.get_text(\"\",strip=True)\n",
    "#         article=''.join(article.split())\n",
    "#         author=author.find_next_sibling('div',class_='text_c').get_text(\"\",strip=True)\n",
    "#         author=''.join(author.split())\n",
    "#\n",
    "#     print('网页标题：',title)\n",
    "#\n",
    "#     #提取出的网页内容存在title,article,author三个字符串里，对它们进行中文分词\n",
    "#     seggen=jieba.cut_for_search(title)\n",
    "#     seglist=list(seggen)\n",
    "#     seggen=jieba.cut_for_search(article)\n",
    "#     seglist+=list(seggen)\n",
    "#     seggen=jieba.cut_for_search(author)\n",
    "#     seglist+=list(seggen)\n",
    "#\n",
    "#     #数据存储\n",
    "#     # conn=sqlite3.connect(\"viewsdu.db\")\n",
    "#     # c=conn.cursor()\n",
    "#     # c.execute('insert into doc values(?,?)',(cnt,url))\n",
    "#\n",
    "#     #对每个分出的词语建立词表\n",
    "#     for word in seglist:\n",
    "#         #print(word)\n",
    "#         #检验看看这个词语是否已存在于数据库\n",
    "#         # c.execute('select list from word where term=?',(word,))\n",
    "#         # result=c.fetchall()\n",
    "#         #如果不存在\n",
    "#         # if len(result)==0:\n",
    "#         #     docliststr=str(cnt)\n",
    "#         #     c.execute('insert into word values(?,?)',(word,docliststr))\n",
    "#         #如果已存在\n",
    "#         # else:\n",
    "#         #     docliststr=result[0][0]#得到字符串\n",
    "#         #     docliststr+=' '+str(cnt)\n",
    "#         #     c.execute('update word set list=? where term=?',(docliststr,word))\n",
    "#         print(word)\n",
    "#     # conn.commit()\n",
    "#     # conn.close()\n",
    "#\n",
    "#     print('词表建立完毕=======================================================')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T13:02:53.152553Z",
     "start_time": "2024-05-26T13:02:50.315170200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI\n\u001B[0;32m      2\u001B[0m chat \u001B[38;5;241m=\u001B[39m ChatOpenAI(temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m)\n\u001B[0;32m      3\u001B[0m chat\n",
      "File \u001B[1;32mD:\\system_default\\desktop\\MLTW\\venv\\Lib\\site-packages\\langchain\\chat_models\\__init__.py:27\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 27\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m chat_models\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# If not in interactive env, raise warning.\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_interactive_env():\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T03:48:59.874817600Z",
     "start_time": "2024-05-29T03:48:59.808710300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='sk-proj-TszsCCtM1AieTmUN9Y3QT3BlbkFJVTso6p30sr4wuBSHis4s', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
