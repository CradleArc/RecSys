{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t3['kk'].loc[(t3['is_intact'] == 1) & (t3['serialno'] == 26)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 负采样函数，这里可以控制负采样时的比例, 这里给了一个默认的值\n",
    "# def neg_sample_recall_data(recall_items_df, sample_rate=0.1):\n",
    "#     pos_data = recall_items_df[recall_items_df['label'] == 1]\n",
    "#     neg_data = recall_items_df[recall_items_df['label'] == 0]\n",
    "\n",
    "#     print('pos_data_num:', len(pos_data), 'neg_data_num:', len(neg_data), 'pos/neg:', len(pos_data)/len(neg_data))\n",
    "\n",
    "#     # def neg_sample_func(group_df):\n",
    "#     #     neg_num = len(group_df)\n",
    "#     #     sample_num = max(int(neg_num * sample_rate), 6) \n",
    "#     #     sample_num = min(sample_num, 6) # 保证最多不超过5个，这里可以根据实际情况进行选择\n",
    "#     #     return group_df.sample(n=sample_num, replace=True)\n",
    "\n",
    "#     # neg_data_user_sample = neg_data.groupby('did').apply(neg_sample_func)\n",
    "#     neg_data_did_sample = neg_data.groupby('did',group_keys=False).sample(6)\n",
    "#     # neg_data_item_sample = neg_data.groupby('candi_vid', group_keys=False).apply(neg_sample_func)\n",
    "#     neg_data_vid_sample = neg_data.groupby('candi_vid',group_keys=False).sample(1)\n",
    "\n",
    "#     neg_data_new = neg_data_did_sample.append(neg_data_vid_sample)\n",
    "#     neg_data_new = neg_data_new.sort_values(['did','candi_vid']).drop_duplicates(['did','candi_vid'], keep='last')\n",
    "\n",
    "#     # 将正样本数据合并\n",
    "#     data_new = pd.concat([pos_data, neg_data_new], ignore_index=True)\n",
    "\n",
    "#     return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 哇，这是一个伟大的写法\n",
    "# tt = trn_user_cid_recall_df.groupby('did',group_keys=False).sample(6)\n",
    "# tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_vid_item_label_df = reduce_mem(trn_vid_item_label_df)\n",
    "# val_vid_item_label_df = reduce_mem(val_vid_item_label_df)\n",
    "# tst_vid_item_label_df = reduce_mem(tst_vid_item_label_df)\n",
    "# # all_vid_item_label_df = reduce_mem(all_vid_item_label_df)\n",
    "\n",
    "# ## 用于验证的数据\n",
    "# local_train_log = reduce_mem(local_train_log)\n",
    "# local_valid_log = reduce_mem(local_valid_log)\n",
    "# local_test_log = reduce_mem(local_test_log)\n",
    "\n",
    "# ## 原始的数据\n",
    "# local_train = reduce_mem(local_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 类别标签编码\n",
    "# cate_cols = ['V_is_intact',\n",
    "#              'V_classify_id',\n",
    "#              'V_series_id',\n",
    "#              'V_upgc_flag',\n",
    "#              # 'V_follow',\n",
    "             \n",
    "#              'V_cid',\n",
    "#              'U_cid_like_hist',\n",
    "#              'U_cid_like_rect', # 已经标签编码了\n",
    "#              'U_series_id',\n",
    "#              'U_classify_id',\n",
    "#              'U_is_intact',\n",
    "#              'U_upgc_flag',\n",
    "#              ]\n",
    "\n",
    "# for cat in cate_cols:\n",
    "#     trn_user_recall_feature[cat] = trn_user_recall_feature[cat].astype('category')\n",
    "#     val_user_recall_feature[cat] = val_user_recall_feature[cat].astype('category')\n",
    "#     tst_user_recall_feature[cat] = tst_user_recall_feature[cat].astype('category')\n",
    "\n",
    "# # 这里有一个不错的想法；历史中构成序列的两个合集用后面的一个作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # user_did = user_cid_info.did.unique()\n",
    "    # # 关注近期有效时间的视频观看\n",
    "    # udhi = dict()\n",
    "    # for did,vid_log in tqdm(user_cid_info.groupby('did')):\n",
    "    #     time_c = 0\n",
    "    #     # 默认是 10 条视频\n",
    "    #     vid_number = 0\n",
    "    #     for i in range(len(vid_log)):\n",
    "    #         time_c += vid_log.iloc[i].time_gap\n",
    "    #         # 最近3小时用户的日志记录\n",
    "    #         if time_c >= 10800: \n",
    "    #             vid_number = i\n",
    "    #             break\n",
    "    #     if vid_number == 0:\n",
    "    #         udhi[did] = vid_log\n",
    "    #     else:\n",
    "    #         udhi[did] = vid_log.head(vid_number)\n",
    "\n",
    "    # # 字典转化为 pd : did、duration\n",
    "    # user_cid_time_info = []\n",
    "    # for i in tqdm(range(len(user_did))):\n",
    "    #     did = user_did[i]\n",
    "    #     eff_time_vid = udhi[did]\n",
    "    #     for j in range(len(eff_time_vid)):\n",
    "    #         user_cid_time_info.append((did,eff_time_vid.iloc[j].cid))\n",
    "\n",
    "    # user_cid_time_info = pd.DataFrame(user_cid_time_info,columns=['did','cid'])\n",
    "    # # ——————————————————————————————————————————————————————————————————————————————————\n",
    "    # # 用户的 近期合集爱好和历史合集爱好\n",
    "    # del user_cid_info['time_gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['did','vid','vts','hb','seq_no','cpn','fpn','time_gap']\n",
    "# train_limit = nature_time_limit(train.copy(),cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit.to_csv('./data_v2/train_limit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit = pd.read_csv('./data_v2/train_limit.csv')\n",
    "# train_limit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit.did.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit = train_limit.merge(vid_info,on='vid',how='left')\n",
    "# train_limit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['eff_watch'] = train['vts'] / train['duration']\n",
    "# train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit['cid_count'] = train_limit['cid'].map(train_limit.cid.value_counts())\n",
    "# train_limit['vid_count'] = train_limit['vid'].map(train_limit.vid.value_counts())\n",
    "# train_limit['cid_own_vid'] = train_limit['cid'].map(vid_info.cid.value_counts())\n",
    "# train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit = train_limit.merge(train,on=['did','vid','vts','hb','seq_no','cpn','fpn','time_gap'],how='left')\n",
    "# # del train_limit['time_gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_limit = train_limit[['did','vid','cid','seq_no','serialno','is_intact','classify_id','series_id','duration','vts','hb','time_gap','cpn','fpn','title_length','upgc_flag','user_cid_n','cid_count','vid_count','cid_own_vid']]\n",
    "# train_limit.sort_values(by=['did','seq_no'],ascending=False,inplace=True)\n",
    "# train_limit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del train_limit['user_cid_n']\n",
    "#         df1['stars_sim'] =  len([x for x in df2 if x in df1])\n",
    "#         df3['tags_sim'] = len([x for x in df4 if x in df3])\n",
    "#         # df5['stars_sim'] = len([x for x in df6 if x in df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = train_limit.groupby(['did','cid']).size().sort_values(ascending=False).reset_index()\n",
    "# tmp.columns = ['did','cid','user_cid_n']\n",
    "# train_limit = train_limit.merge(tmp[['did','cid','user_cid_n']],on=['did','cid'],how='left')\n",
    "# train_limit.sort_values(['did','seq_no'],inplace=True,ascending=False)\n",
    "# del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid_info['vid_pop'] = vid_info['vid'].map(train_limit.vid.value_counts())\n",
    "# vid_info['cid_pop'] = vid_info['cid'].map(train_limit.cid.value_counts())\n",
    "# vid_info.sort_values(by=['vid_pop'],ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 后面的训练选择 6 条数据以上的,现在的数据是所有用户一周的日志记录\n",
    "# train_limit['did_count'] = train_limit['did'].map(train_limit.did.value_counts())\n",
    "# kk = train_limit[train_limit['did_count'] >= 6]\n",
    "# num = np.random.choice(kk.did.unique(),size=5000,replace=False)\n",
    "# sample = kk[kk['did'].isin(num)]\n",
    "# del train_limit['did_count']\n",
    "# del sample['did_count']\n",
    "# del kk\n",
    "# sample.did.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nature_time_limit(train,cols):\n",
    "    \n",
    "#     user_did = train.did.unique()\n",
    "#     tl = dict()\n",
    "#     for did,vid_log in tqdm(train.groupby('did')):\n",
    "#         time_c = 0\n",
    "#         vid_number = 0\n",
    "#         for i in range(len(vid_log)):\n",
    "#             time_c += vid_log.iloc[i].time_gap\n",
    "#             # 最近一周用户的日志记录\n",
    "#             if time_c >= 604800: \n",
    "#                 vid_number = i\n",
    "#                 break\n",
    "#         if vid_number == 0:\n",
    "#             tl[did] = vid_log\n",
    "#         elif vid_number < 4:\n",
    "#             tl[did] = vid_log.iloc[0:4]\n",
    "#         else:\n",
    "#             tl[did] = vid_log.iloc[0:vid_number+1]\n",
    "\n",
    "#     train_limit = []\n",
    "#     for i in tqdm(range(len(user_did))):\n",
    "#         did = user_did[i]\n",
    "#         eff_vid = tl[did]\n",
    "#         for j in range(len(eff_vid)):\n",
    "#             train_limit.append((did,eff_vid.iloc[j].vid,\n",
    "#                                     eff_vid.iloc[j].vts,\n",
    "#                                     eff_vid.iloc[j].hb,\n",
    "#                                     eff_vid.iloc[j].seq_no,\n",
    "#                                     eff_vid.iloc[j].cpn,\n",
    "#                                     eff_vid.iloc[j].fpn,\n",
    "#                                     eff_vid.iloc[j].time_gap,\n",
    "#                                     ))\n",
    "\n",
    "#     train_limit = pd.DataFrame(train_limit,columns=['did','vid','vts','hb','seq_no','cpn','fpn','time_gap'])\n",
    "\n",
    "#     return train_limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_id : 55601 0 106049 51680 105454 19033 0 0 \n",
    "#           额外： 55466 19033 0 103541 0 55466 103541 97644 \n",
    "# 0 号 让我感觉是 新闻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def user_last_serial_fea(recall_feature,user_log):\n",
    "    \n",
    "#     # 得先排好序\n",
    "#     user_last = user_log.groupby('did').head(1)\n",
    "#     last_vid = dict()\n",
    "#     # 字典要快很多\n",
    "#     for did,check_ in tqdm(user_last.groupby('did')):\n",
    "#         last_vid[did] = check_ \n",
    "\n",
    "#     # 初始化全部的 后续视频为 0 \n",
    "#     recall_feature['follow'] = 0\n",
    "#     recall_feature['collect'] = 0\n",
    "\n",
    "#     def change(recall_df):\n",
    "#         did = recall_df.iloc[0].did\n",
    "#         last_l = last_vid[did].iloc[0]\n",
    "#         recall_df['follow'].loc[(recall_df['classify_id'] == last_l.classify_id) & (recall_df['series_id'] == last_l.series_id) &\n",
    "#             (recall_df['cid'] == last_l.cid) & \n",
    "#             # (recall_df['is_intact'] == last_l.is_intact) & \n",
    "#             (recall_df.serialno - last_l.serialno == 1)] = 1 \n",
    "\n",
    "#         recall_df['collect'].loc[(recall_df['classify_id'] == last_l.classify_id) & (recall_df['series_id'] == last_l.series_id) &\n",
    "#             (recall_df['cid'] == last_l.cid) & (recall_df['is_intact'] == last_l.is_intact)] = 1\n",
    "#         return recall_df\n",
    "\n",
    "#     recall_feature = recall_feature.groupby('did').apply(change)\n",
    "\n",
    "#     return recall_feature\n",
    "               \n",
    "# trn_vid_fea = user_last_serial_fea(trn_vid_fea.copy(),local_train)\n",
    "# val_vid_fea = user_last_serial_fea(val_vid_fea.copy(),local_valid)\n",
    "# tst_vid_fea = user_last_serial_fea(tst_vid_fea.copy(),local_test)\n",
    "\n",
    "# trn_vid_fea.follow.value_counts()\n",
    "# # trn_user_dura_info = user_video_dura_fea(local_train,user_duration_hot_cols)\n",
    "# # val_user_dura_info = user_video_dura_fea(local_valid,user_duration_hot_cols)\n",
    "# # tst_user_dura_info = user_video_dura_fea(local_test,user_duration_hot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_user_recall_feature['push_score'] = trn_user_recall_feature['cand_cid_number'] / trn_user_recall_feature['cid_number']\n",
    "# val_user_recall_feature['push_score'] = val_user_recall_feature['cand_cid_number'] / val_user_recall_feature['cid_number']\n",
    "# tst_user_recall_feature['push_score'] = tst_user_recall_feature['cand_cid_number'] / tst_user_recall_feature['cid_number']\n",
    "# trn_user_recall_feature.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 限制后的周条目发生了改变，需要重新计算\n",
    "# del sample_limit['user_cid_n']\n",
    "# tmp = sample_limit.groupby(['did','cid']).size().sort_values(ascending=False).reset_index()\n",
    "# tmp.columns = ['did','cid','user_cid_n']\n",
    "# sample_limit = sample_limit.merge(tmp[['did','cid','user_cid_n']],on=['did','cid'],how='left')\n",
    "# sample_limit.sort_values(['did','seq_no'],inplace=True,ascending=False)\n",
    "# del tmp\n",
    "# sample_limit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # 历史观看最多，加以限制为一周\n",
    "    # user_rect_watch = user_rect_watch.head(1)\n",
    "\n",
    "    # user_most_watch = user_most_watch.drop_duplicates(['cid'],keep='first')\n",
    "    # user_10_watch = user_most_watch[user_most_watch['user_cid_n'] >= 10]\n",
    "    # user_most_watch = user_most_watch[(user_most_watch['user_cid_n'] < 10) & (user_most_watch['user_cid_n'] > 1)].head(3)\n",
    "\n",
    "    # candi_cid = user_most_watch.append(user_rect_watch)\n",
    "    # # candi_cid = candi_cid.append(user_10_watch)\n",
    "    # candi_cid = candi_cid.drop_duplicates('cid')\n",
    "\n",
    "    # # 2、合集召回\n",
    "    # for i in range(len(candi_cid)):\n",
    "    #     cid_i = candi_cid.iloc[i]\n",
    "    #     # 配置信息\n",
    "    #     user_cid = cid_i.cid\n",
    "    #     user_series = cid_i.series_id\n",
    "    #     user_classify = cid_i.classify_id\n",
    "    #     # 规则约束条件，对召回的视频合集筛选\n",
    "    #     user_cid_rec = vid_info_cand[(vid_info_cand['cid'] == user_cid) &\n",
    "    #                             (vid_info_cand['series_id'] == user_series) &\n",
    "    #                             (vid_info_cand['classify_id'] == user_classify)]\n",
    "    #     if len(user_cid_rec) > 100:\n",
    "    #         user_cid_rec = user_cid_rec[user_cid_rec['vid_pop'] > 500]\n",
    "\n",
    "    #     recall = recall.append(user_cid_rec['candi_vid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ntl(group_df):\n",
    "#     time_c = 0\n",
    "#     vid_number = 0\n",
    "#     for i in range(len(group_df)):\n",
    "#         time_c += group_df.iloc[i].time_gap \n",
    "#         if time_c >= 604800:\n",
    "#             vid_number = i \n",
    "#             break \n",
    "#     if vid_number == 0:\n",
    "#         return group_df \n",
    "#     elif vid_number < 4:\n",
    "#         return group_df.head(4)\n",
    "#     else:\n",
    "#         return group_df.head(vid_number+1)\n",
    "# print('不加一周时间限制的数据大小',sample.shape)\n",
    "# sample_limit = sample.groupby('did',group_keys=False).apply(ntl)\n",
    "# print('加一周时间限制的数据大小',sample_limit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 用户近期的10条视频日志系列和类型爱好,加入自然时间限制\n",
    "# def user_video_style_fea(train,cols):\n",
    "\n",
    "#     user_video = train[cols]\n",
    "#     # user_video_time_info = user_video_time_info.groupby('did').head(6)\n",
    "\n",
    "#     user_video.rename(columns={'series_id':'U_series_id',\n",
    "#                                     'classify_id':'U_classify_id',\n",
    "#                                     'is_intact':'U_is_intact',\n",
    "#                                     'upgc_flag':'U_upgc_flag'},inplace=True)\n",
    "\n",
    "#     user_video = user_video.groupby('did').agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "#     user_video_1 = user_video.groupby('did').head(1)\n",
    "#     user_video['U_csi'] = user_video['U_classify_id'].astype('str') + '_' + user_video['U_series_id'].astype('str') + '_' + user_video['U_is_intact'].astype('str')\n",
    "    \n",
    "#     user_video['U_csi_last'] = user_video_1['U_classify_id'].astype('str') + '_' + user_video_1['U_series_id'].astype('str') + '_' + user_video_1['U_is_intact'].astype('str')\n",
    "#     return user_video[['did','U_csi','U_csi_last','U_upgc_flag']]\n",
    "\n",
    "# user_style_hot_cols = ['did','series_id','classify_id','is_intact','upgc_flag']\n",
    "\n",
    "# trn_user_sty_info = user_video_style_fea(local_train_4,user_style_hot_cols)\n",
    "# val_user_sty_info = user_video_style_fea(local_valid_4,user_style_hot_cols)\n",
    "# tst_user_sty_info = user_video_style_fea(local_test_4,user_style_hot_cols)\n",
    "# # all_user_sty_info = user_video_style_fea(train_4,user_style_hot_cols)\n",
    "\n",
    "# print(trn_user_sty_info.shape)\n",
    "# trn_user_sty_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaler(group_df):\n",
    "#     if group_df.iloc[0] > group_df.iloc[1]:\n",
    "#         group_df.iloc[0] = 2 * group_df.iloc[1] - group_df.iloc[2]\n",
    "#     return group_df \n",
    "# k['score'] = k.groupby('vid',group_keys=False)['score'].apply(scaler)\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 序列大小为2 的特征重组\n",
    "# def seq_feature(data_,vid_info_):\n",
    "   \n",
    "#     data_ = data_.groupby('did').head(2)\n",
    "#     if len(data_) != 170909 * 2:\n",
    "#         return \n",
    "#     data_ = data_.merge(vid_info[['vid','duration']],on='vid',how='left')\n",
    "#     # 用户\n",
    "#     data_['wr'] = data_['vts'] / data_['duration']\n",
    "#     data_['fr'] = data_['hb'] / data_['duration']\n",
    "#     del data_['duration']\n",
    "    \n",
    "#     data_ = data_.sort_values(by=['did','seq_no'],ascending=False)\n",
    "#     data_['next_vid'] = data_.groupby('did').vid.shift(1)\n",
    "#     data_['next_cpn'] = data_.groupby('did').cpn.shift(1)\n",
    "#     data_['next_fpn'] = data_.groupby('did').fpn.shift(1)\n",
    "#     data_['next_wr'] = data_.groupby('did').wr.shift(1)\n",
    "#     data_['next_fr'] = data_.groupby('did').fr.shift(1)\n",
    "#     data_['next_time_gap'] = data_.groupby('did').time_gap.shift(1)\n",
    "#     # data_['next_stars'] = data_.groupby('did').stars.shift(1)\n",
    "#     # data_['next_tags'] = data_.groupby('did').tags.shift(1)\n",
    "#     # data_['next_key_word'] = data_.groupby('did').key_word.shift(1)\n",
    "\n",
    "#     ########################################################## 这条特征应该在召回之后 ############################################################\n",
    "#     # # 当前视频的热度分数\n",
    "#     # data_['vid_score'] = data_['vid'].map(data_.vid.value_counts())\n",
    "#     # # 下一视频的热度分数\n",
    "#     # nv = data_.groupby(['vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "#     # nv = nv[nv.next_vid.isin(candidate_items.vid.unique())]\n",
    "#     # nv.rename(columns={0:'next_vid_score'},inplace=True)\n",
    "\n",
    "#     # # 用户下一视频观看的概率，但用所有日志数据做统计，得到下一视频观看的热度\n",
    "#     # data_ = data_.merge(nv,on=['vid','next_vid'],how='left')\n",
    "#     ###########################################################################\n",
    "\n",
    "#     # 当前视频的信息\n",
    "#     data_ = data_.merge(vid_info_,on='vid',how='left')\n",
    "#     # 下一视频的信息\n",
    "#     vid_info_n = vid_info_.copy() \n",
    "#     vid_info_n.rename(columns={'vid':'next_vid','cid':'n_cid','is_intact':'n_is_intact','serialno':'n_serialno','classify_id':'n_classify_id',\n",
    "#                                'series_id':'n_series_id','duration':'n_duration','title_length':'n_title_length','upgc_flag':'n_upgc_flag',\n",
    "#                                'stars':'n_stars','tags':'n_tags','key_word':'n_key_word'},inplace=True)\n",
    "#     data_ = data_.merge(vid_info_n,on='next_vid',how='left')\n",
    "\n",
    "#     print(\"———————————————————————————————————————————— 序列特征抽离 ————————————————————————————————————————\")\n",
    "#     data_ = data_.groupby('did').tail(1)\n",
    "\n",
    "#     # 差值并不好，用均值可能会更好 apply(lambda x: 1 if x else 0)\n",
    "#     # 用户侧的日志序列特征\n",
    "#     data_['seq_cpn_same'] = (data_['next_cpn'].values == data_['cpn'].values) * 1\n",
    "#     data_['seq_fpn_same'] = (data_['next_fpn'].values == data_['fpn'].values) * 1\n",
    "#     data_['seq_time_diff'] = (abs(data_['next_time_gap'].values - data_['time_gap'].values)) / 60   # 这里的 time_gap 可能需要调整,不管了，按照 time_gap - vts 的话存在一些错误 (时间单位 ： minute)\n",
    "#     data_['seq_wr_favor'] = (data_['next_wr'].values + data_['wr'].values) / 2\n",
    "#     data_['seq_fr_favor'] = (data_['next_fr'].values + data_['fr'].values) / 2 \n",
    "#     # 视频侧的比较特征\n",
    "#     data_['seq_cid_same'] = (data_['n_cid'].values == data_['cid'].values) * 1\n",
    "#     data_['seq_intact_same'] = (data_['n_is_intact'].values == data_['is_intact'].values) * 1\n",
    "#     data_['seq_serialno_up'] = (data_['n_serialno'].values == (data_['serialno'].values + 1)) * 1\n",
    "#     data_['seq_classify_same'] = (data_['n_classify_id'].values == data_['classify_id'].values) * 1\n",
    "#     data_['seq_series_same'] = (data_['n_series_id'].values == data_['series_id'].values) * 1\n",
    "#     data_['seq_dura_diff'] = data_['n_duration'].values - data_['duration'].values\n",
    "#     data_['seq_title_diff'] = data_['n_title_length'].values - data_['title_length'].values \n",
    "\n",
    "#     data_['seq_stars_same'] = data_['n_stars'].values & data_['stars'].values\n",
    "#     data_['seq_tags_same'] = data_['n_tags'].values & data_['tags'].values\n",
    "#     data_['seq_key_same'] = data_['n_key_word'].values & data_['key_word'].values\n",
    "\n",
    "#     data_['seq_stars_same'] = data_['seq_stars_same'].apply(lambda x:len(x))\n",
    "#     data_['seq_tags_same'] = data_['seq_tags_same'].apply(lambda x:len(x))\n",
    "#     data_['seq_key_same'] = data_['seq_key_same'].apply(lambda x:len(x))\n",
    "\n",
    "#     data_ = data_[['did','seq_cpn_same','seq_fpn_same','seq_time_diff','seq_wr_favor','seq_fr_favor',\n",
    "#                   'seq_cid_same','seq_intact_same','seq_serialno_up','seq_classify_same','seq_series_same','seq_dura_diff','seq_title_diff',\n",
    "#                   'seq_stars_same','seq_tags_same','seq_key_same']]\n",
    "#     data_ = reduce_mem(data_)\n",
    "#     gc.collect()\n",
    "#     return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def sim_score(df):\n",
    "    #     did = df.iloc[0].did \n",
    "    #     df['stars_sim'] = df['stars'].apply(lambda x: stars_h[did][x])\n",
    "    # #     df['tags_sim'] = df['tags'].apply(lambda x: tags_h[did][x])\n",
    "    # #     df['key_word_sim'] = df['key_word'].apply(lambda x: key_word_h[did][x])\n",
    "    #     # return df[['stars_sim','tags_sim','key_word_sim']]\n",
    "    #     return df['stars_sim']\n",
    "    #         stars_h = dict() \n",
    "    # tags_h = dict() \n",
    "    # key_word_h = dict()\n",
    "    # for did,hist in tqdm(data_.groupby('did')):\n",
    "    #     stars_h[did] = Counter(hist.iloc[0].stars)\n",
    "    #     # tags_h[did] = Counter(hist.iloc[0].tags)\n",
    "    #     # key_word_h[did] = Counter(hist.iloc[0].key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stat_feature(data_,recall_data_,vid_info_,candidate_items_):\n",
    "    \n",
    "#     data_['next_vid'] = data_.groupby(['did']).vid.shift(1)\n",
    "#     data_ = data_.merge(vid_info,on='vid',how='left')\n",
    "#     d_ = data_.groupby('did').head(1)\n",
    "#     recall_data_ = recall_data_.merge(d_[['did','vid','cid','fpn']],on='did',how='left')\n",
    "#     data_ = data_.sort_values(by=['did','seq_no'],ascending=False)\n",
    "#     # 特征处理列\n",
    "#     sta_cols = ['did','vid','next_vid','cid','fpn']\n",
    "#     sta_cols_ = ['did','vid','cid','candi_vid','fpn']\n",
    "\n",
    "#     data_ = data_[sta_cols]\n",
    "#     recall_data_ = recall_data_[sta_cols_]\n",
    "\n",
    "#     print(\"———————————————————————————— 我的无敌四大法宝 ——————————————————————————\")\n",
    "#     ################### 规则一号\n",
    "#     vid_habit = data_.groupby(['vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "#     vid_habit = vid_habit[vid_habit.next_vid.isin(candidate_items_.vid.unique())]\n",
    "#     vid_habit.rename(columns={0:'sta_score','next_vid':'candi_vid'},inplace=True)\n",
    "\n",
    "#     #################### 规则三号\n",
    "#     # cpn_habit = data_.groupby(['cpn','vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "#     # cpn_habit = cpn_habit[cpn_habit.next_vid.isin(candidate_items_.vid.unique())]\n",
    "#     # cpn_habit.rename(columns={0:'cpn_score','next_vid':'candi_vid'},inplace=True)\n",
    "\n",
    "#     fpn_habit = data_.groupby(['fpn','vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "#     fpn_habit = fpn_habit[fpn_habit.next_vid.isin(candidate_items_.vid.unique())]\n",
    "#     fpn_habit.rename(columns={0:'fpn_score','next_vid':'candi_vid'},inplace=True)\n",
    "\n",
    "\n",
    "#     recall_data_ = recall_data_.merge(vid_habit,on=['vid','candi_vid'],how='left')\n",
    "\n",
    "#     recall_data_ = recall_data_.merge(fpn_habit,on=['fpn','vid','candi_vid'],how='left')\n",
    "\n",
    "#     recall_data_ = recall_data_.fillna(0)\n",
    "#     recall_data_ = recall_data_[['did','candi_vid','sta_score','fpn_score']]\n",
    "\n",
    "#     recall_data_ = reduce_mem(recall_data_)\n",
    "#     gc.collect()\n",
    "#     return recall_data_\n",
    "\n",
    "# train_sta_f = stat_feature(train_data_7,train_recall,vid_info,candidate_items)\n",
    "# train_sta_f.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_user_cid_recall_df = user_cid_recall_df_func(trn_user_cid_recall,local_train_log)\n",
    "# print(trn_user_cid_recall_df.shape)\n",
    "# val_user_cid_recall_df = user_cid_recall_df_func(val_user_cid_recall,local_train_log)\n",
    "# print(val_user_cid_recall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_cid_recall_hit_score_func(trn_user_cid_recall,local_train_log)\n",
    "\n",
    "# user_cid_recall_hit_score_func(val_user_cid_recall,local_valid_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 重排序用于训练的日志文件\n",
    "# local_train = local_train.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "# local_valid = local_valid.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "# local_test = local_test.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "# # 重排序用于验证模型预测的日志文件\n",
    "# local_train_log = local_train_log.sort_values(by=['did']).reset_index(drop=True)\n",
    "# local_valid_log = local_valid_log.sort_values(by=['did']).reset_index(drop=True)\n",
    "# local_test_log = local_test_log.sort_values(by=['did']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_train = local_train.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "# local_valid = local_valid.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "# local_test = local_test.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)\n",
    "# sample = train.sort_values(by=['did','seq_no'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 我的二号小字典已经准备就绪随时跳伞  (vid -> cid -> next_vid -> score) , 这个字典要比前面的那个字典要更为精确\n",
    "# # 并且这个就相当于是 物品协同过滤 和 合集召回\n",
    "# cid_habit = train.groupby(['cid','vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "# cid_habit = cid_habit[cid_habit.next_vid.isin(candidate_items.vid.unique())]\n",
    "# cid_habit.rename(columns={0:'score','next_vid':'candi_vid'},inplace=True)\n",
    "# cid_habit.sort_values(by=['cid','vid','score'],ascending=False)\n",
    "# cid_habit = cid_habit.groupby(['cid','vid']).head(20) \n",
    "# print(len(cid_habit))\n",
    "# # 做成字典， come on\n",
    "# cid_action = dict() \n",
    "# for vid,cand in tqdm(cid_habit.groupby('cid')):\n",
    "#     cid_action[vid] = cand\n",
    "# len(cid_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 我的 三号小字典已经准备就绪跳伞 结构 : (cpn -> cid -> vid -> next_vid -> score)\n",
    "# # 这个相当于是 页面合集召回\n",
    "# cpn_habit = train.groupby(['cpn','vid','next_vid']).size().sort_values(ascending=False).reset_index()\n",
    "# cpn_habit = cpn_habit[cpn_habit.next_vid.isin(candidate_items.vid.unique())]\n",
    "# cpn_habit.rename(columns={0:'cpn_score','next_vid':'candi_vid'},inplace=True)\n",
    "# cpn_habit.sort_values(by=['cpn','vid','cpn_score'],ascending=False)\n",
    "# cpn_habit = cpn_habit.groupby(['cpn','vid']).head(20)\n",
    "# print(cpn_habit.shape)\n",
    "# cpn_action = dict()\n",
    "# for cpn,cand in tqdm(cpn_habit.groupby('cpn')):\n",
    "#     cpn_action[cpn] = cand\n",
    "# len(cpn_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_recall = dict()\n",
    "# min_recall = 100000\n",
    "# for did, hist in tqdm(local_data.groupby('did')):\n",
    "#     lv_cla = hist.iloc[0].classify_id\n",
    "#     recall = hot_vid_cand[(hot_vid_cand['classify_id'] == lv_cla) & (~hot_vid_cand['vid'].isin(hist['vid'].unique()))].head(10)\n",
    "#     user_recall[did] = recall['vid'] \n",
    "\n",
    "#     min_recall = min(min_recall,len(recall))\n",
    "\n",
    "# print(min_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 我的小字典,这个步骤上分很多,主要靠这个\n",
    "# train['candi_vid'] = train.groupby(['did']).vid.shift(1)\n",
    "# vid_habit = train.groupby(['vid','candi_vid']).size().sort_values(ascending=False).reset_index()\n",
    "# vid_habit = vid_habit[vid_habit.candi_vid.isin(candidate_items.vid.unique())]\n",
    "# del train['candi_vid']\n",
    "# vid_habit.sort_values(by=['vid','candi_vid',0],ascending=False)\n",
    "# vid_habit.rename(columns={0:'pop_score'},inplace=True)\n",
    "# vid_habit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # print(\"——————————————————— 追剧字典 ————————————————————\")\n",
    "    # user_recall = dict()\n",
    "    # for did, hist in tqdm(data.groupby('did')):\n",
    "    #     l_cid = hist.iloc[0].cid \n",
    "    #     recall = vid_info_cand[(vid_info_cand['cid'] == l_cid)]\n",
    "    #     recall = recall[~recall['candi_vid'].isin(hist['vid'].unique())]\n",
    "    #     if len(recall) > 100:\n",
    "    #         recall = recall.sort_values(by=['vid_pop'],ascending=False).head(50)\n",
    "    #     user_recall[did] = recall['candi_vid'] \n",
    "\n",
    "    # cla_habit = data.groupby(['classify_id','vid']).size().sort_values(ascending=False).reset_index()\n",
    "    # cla_habit = cla_habit[cla_habit.vid.isin(candidate_items_.vid.unique())]\n",
    "    # cla_habit = cla_habit.sort_values(by=['classify_id','vid',0],ascending=False)\n",
    "    # cla_habit.rename(columns={0:'cla_score','vid':'candi_vid'},inplace=True)\n",
    "    # # 规则字典\n",
    "    # cla_action = dict()\n",
    "    # for cla_id,cand in tqdm(cla_habit.groupby('classify_id')):\n",
    "    #     cla_action[cla_id] = cand['candi_vid']\n",
    "\n",
    "    ################### 视频表 #####################\n",
    "    # vid_info_cand = vid_info_[vid_info_.vid.isin(candidate_items_.vid.unique())]\n",
    "    # vid_info_cand['vid_pop'] = vid_info_cand['vid'].map(data.vid.value_counts())\n",
    "    # vid_info_cand.rename(columns={'vid':'candi_vid'},inplace=True)\n",
    "    # vid_info_cand = vid_info_cand.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaler(group_df):\n",
    "#     if group_df.iloc[0] > group_df.iloc[1] & len(group_df) > 2:\n",
    "#         group_df.iloc[0] = 2 * group_df.iloc[1] - group_df.iloc[2]\n",
    "#     return group_df \n",
    "# # k['score'] = k.groupby('vid',group_keys=False)['score'].apply(scaler)\n",
    "# vid_habit['pop_score'] = vid_habit.groupby('vid',group_keys=False)['pop_score'].apply(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_vid_cand = train.groupby(['classify_id','vid']).size().sort_values(ascending=False).reset_index()\n",
    "# hot_vid_cand = hot_vid_cand[hot_vid_cand['vid'].isin(candidate_items.vid.unique())].reset_index(drop=True)\n",
    "# hot_vid_cand.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def turn_page(group_df):\n",
    "    #     if len(group_df) == 1:\n",
    "    #         return group_df\n",
    "    #     if group_df.iloc[0].cpn != group_df.iloc[1].cpn | group_df.iloc[0].fpn != group_df.iloc[1].fpn:\n",
    "    #         group_df['U_page_turn'] = 1\n",
    "\n",
    "        # return group_df.head(1) \n",
    "\n",
    "    # user_video_p = user_video_page.groupby('did',group_keys=False).apply(turn_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 焯，写个循环来找最好的随机种子\n",
    "# good_seed = []\n",
    "# tst_log = local_final_log[local_final_log['did'].isin(num3)]\n",
    "# for i in range(1,1000):\n",
    "#     # 排序模型定义\n",
    "#     lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt',\n",
    "#                         objective='binary',\n",
    "#                         num_leaves=301,\n",
    "#                         reg_alpha=0.0,\n",
    "#                         reg_lambda=1,\n",
    "#                         max_depth=17,\n",
    "#                         n_estimators=100,\n",
    "#                         subsample=0.7,\n",
    "#                         colsample_bytree=0.7,\n",
    "#                         subsample_freq=1,\n",
    "#                         learning_rate=0.015,\n",
    "#                         min_child_weight=3,\n",
    "                        \n",
    "#                         random_state=i,\n",
    "#                         n_jobs= 16,\n",
    "#                         )\n",
    "    \n",
    "#     lgb_ranker.fit(trn_data[lgb_cols], trn_data['label'], group=g_train,\n",
    "#             eval_set=[(val_data[lgb_cols], val_data['label'])], \n",
    "#             eval_group= [g_valid], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', 'auc'], early_stopping_rounds=50,)\n",
    "\n",
    "#     tst_data['pred_score'] = lgb_ranker.predict(tst_data[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "#     tst_data.sort_values(by=['did','pred_score'],ascending=False,inplace=True)\n",
    "#     # 评分函数\n",
    "#     res = mrr_score(tst_data,tst_log,topk=6)\n",
    "#     if res > 0.47675:\n",
    "#         good_seed.append(i)\n",
    "# good_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 提交预测\n",
    "# ## 线上提交，直接杠\n",
    "# all_data_vid_fea['pred_score'] = lgb_ranker.predict(all_data_vid_fea[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "# all_data_vid_fea.sort_values(by=['did','pred_score'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     def get_kfold_users(trn_df, n=5):\n",
    "#         user_ids = trn_df['did'].unique()\n",
    "#         user_set = [user_ids[i::n] for i in range(n)]\n",
    "#         return user_set\n",
    "\n",
    "#     param_grid = {\n",
    "#     \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "#     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "#     \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#     \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "#     \"min_child_weight\": trial.suggest_float(\"min_child_weight\",0.5, 50),\n",
    "#     \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "#     \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "#     \"subsample\": trial.suggest_int(\"subsample\",0.7,0.9),\n",
    "#     \"colsample_bytree\": trial.suggest_float(\"colsample\",0.7,0.9),\n",
    "#     \"random_state\": 1998,\n",
    "#     \"n_jobs\": 16,\n",
    "#     \"boosting_type\":'gbdt',\n",
    "#     \"verbose\":10,\n",
    "#     }\n",
    "\n",
    "#     k_fold = 5\n",
    "#     trn_df = train_data_vid_fea\n",
    "#     user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "#     score_list = []\n",
    "#     score_df = trn_df[['did', 'candi_vid','label']]\n",
    "\n",
    "#     for n_fold, valid_user in enumerate(user_set):\n",
    "#         train_idx = trn_df[~trn_df['did'].isin(valid_user)] # add slide user\n",
    "#         valid_idx = trn_df[trn_df['did'].isin(valid_user)]\n",
    "              \n",
    "#         # 模型及参数的定义\n",
    "#         lgb_Classfication = lgb.LGBMClassifier(objective=\"binary\",**param_grid)  \n",
    "\n",
    "#         # 模型训练\n",
    "#         lgb_Classfication.fit(train_idx[lgb_cols], train_idx['label'],\n",
    "#                         eval_set=[(valid_idx[lgb_cols], valid_idx['label'])], \n",
    "#                         eval_metric=['auc',],early_stopping_rounds=100,\n",
    "#                         callbacks=[\n",
    "#                             LightGBMPruningCallback(trial,'auc')\n",
    "#                         ],\n",
    "#                         )\n",
    "        \n",
    "#         valid_idx['pred_score'] = lgb_Classfication.predict_proba(valid_idx[lgb_cols],\n",
    "#                                             num_iteration=lgb_Classfication.best_iteration_)[:,1]\n",
    "\n",
    "#         score_list.append(valid_idx[['did', 'candi_vid', 'pred_score']])\n",
    "\n",
    "#     score_df_ = pd.concat(score_list, axis=0)\n",
    "#     score_df = score_df.merge(score_df_, how='left', on=['did', 'candi_vid'])\n",
    "\n",
    "#     return mrr_score(score_df,local_final_log,topk=6)\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifyer\")\n",
    "# func = lambda trial: objective(trial)\n",
    "# study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm = MinMaxScaler()\n",
    "# feature_m = ['duration','title_length','U_dura_mean','U_dura_std','U_dura_max','U_dura_min','U_tl_mean',\n",
    "#              'U_vid_count_mean','U_vid_count_max','U_vid_count_min','U_vid_count_std','stars_sim','tags_sim','key_word_sim',\n",
    "#              'score','vid_pop','cpn_score']\n",
    "# for f in feature_m:\n",
    "#     trn_data[f] = mm.fit_transform(trn_data[[f]])\n",
    "#     tst_data[f] = mm.fit_transform(tst_data[[f]])\n",
    "#     all_data_vid_fea[f] = mm.fit_transform(all_data_vid_fea[[f]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "407107e68832157a7bf440b2e842437834c5ebf88d5cafbcbfa00fd7b9d9b746"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
